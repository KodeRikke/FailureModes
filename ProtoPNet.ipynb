{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set path\n",
        "path = \"C:/Users/OhRai/Desktop/IAI/\" #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8z4mEV5rw0S",
        "outputId": "f64f1297-4d8f-46fe-f45d-6dd05382e1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using :  cuda\n"
          ]
        }
      ],
      "source": [
        "path = \"C:/Users/OhRai/Desktop/IAI/\" #\n",
        "\n",
        "# libraries\n",
        "import re\n",
        "from tkinter import Y\n",
        "from collections import Counter\n",
        "import math\n",
        "import os\n",
        "import copy\n",
        "import cv2\n",
        "import heapq\n",
        "import matplotlib.image as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "\n",
        "model_dir = path + 'pretrained_models/'\n",
        "\n",
        "\n",
        "# functions\n",
        "def makedir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        \n",
        "makedir(model_dir)\n",
        "\n",
        "def log(line, file):\n",
        "  with open(path + file, 'a+') as log:\n",
        "      content = log.read()\n",
        "      log.write(content + line + str(\"\\n\"))\n",
        "\n",
        "def list_of_distances(X, Y):\n",
        "    return torch.sum((torch.unsqueeze(X, dim=2) - torch.unsqueeze(Y.t(), dim=0)) ** 2, dim=1)\n",
        "\n",
        "def preprocess(x):\n",
        "    mean, std, y = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225), torch.zeros_like(x)\n",
        "    for i in range(3):\n",
        "        y[:, i, :, :] = (x[:, i, :, :] - mean[i]) / std[i]\n",
        "    return y\n",
        "\n",
        "def undo_preprocess(x):\n",
        "    mean, std, y = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225), torch.zeros_like(x)\n",
        "    for i in range(3):\n",
        "        y[:, i, :, :] = x[:, i, :, :] * std[i] + mean[i]\n",
        "    return y\n",
        "\n",
        "def find_high_activation_crop(activation_map, percentile=95):\n",
        "    threshold = np.percentile(activation_map, percentile)\n",
        "    mask = np.ones(activation_map.shape)\n",
        "    mask[activation_map < threshold] = 0\n",
        "    lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
        "    for i in range(mask.shape[0]):\n",
        "        if np.amax(mask[i]) > 0.5:\n",
        "            lower_y = i\n",
        "            break\n",
        "    for i in reversed(range(mask.shape[0])):\n",
        "        if np.amax(mask[i]) > 0.5:\n",
        "            upper_y = i\n",
        "            break\n",
        "    for j in range(mask.shape[1]):\n",
        "        if np.amax(mask[:,j]) > 0.5:\n",
        "            lower_x = j\n",
        "            break\n",
        "    for j in reversed(range(mask.shape[1])):\n",
        "        if np.amax(mask[:,j]) > 0.5:\n",
        "            upper_x = j\n",
        "            break\n",
        "    return lower_y, upper_y+1, lower_x, upper_x+1\n",
        "\n",
        "def save_prototype_original_img_with_bbox(fname, epoch, index, bbox_height_start, bbox_height_end, bbox_width_start, bbox_width_end):\n",
        "    p_img_bgr = cv2.imread(os.path.join(load_img_dir, 'epoch-'+str(epoch), 'prototype-img-original'+str(index)+'.png'))\n",
        "    cv2.rectangle(p_img_bgr, (bbox_width_start, bbox_height_start), (bbox_width_end-1, bbox_height_end-1), (0, 255, 255), thickness=2)\n",
        "    p_img_rgb = np.float32(p_img_bgr[...,::-1]) / 255\n",
        "    plt.imsave(fname, p_img_rgb)\n",
        "\n",
        "# find nearest\n",
        "def imsave_with_bbox(fname, img_rgb, bbox_height_start, bbox_height_end,\n",
        "                     bbox_width_start, bbox_width_end, color=(0, 255, 255)):\n",
        "    img_bgr_uint8 = cv2.cvtColor(np.uint8(255*img_rgb), cv2.COLOR_RGB2BGR)\n",
        "    cv2.rectangle(img_bgr_uint8, (bbox_width_start, bbox_height_start), (bbox_width_end-1, bbox_height_end-1),\n",
        "                  color, thickness=2)\n",
        "    img_rgb_uint8 = img_bgr_uint8[...,::-1]\n",
        "    img_rgb_float = np.float32(img_rgb_uint8) / 255\n",
        "    plt.imsave(fname, img_rgb_float)\n",
        "\n",
        "class ImagePatch:\n",
        "    def __init__(self, patch, label, distance, original_img=None, act_pattern=None, patch_indices=None):\n",
        "        self.patch = patch\n",
        "        self.label = label\n",
        "        self.negative_distance = -distance\n",
        "        self.original_img = original_img\n",
        "        self.act_pattern = act_pattern\n",
        "        self.patch_indices = patch_indices\n",
        "    def __lt__(self, other):\n",
        "        return self.negative_distance < other.negative_distance\n",
        "\n",
        "class ImagePatchInfo:\n",
        "    def __init__(self, label, distance):\n",
        "        self.label = label\n",
        "        self.negative_distance = -distance\n",
        "    def __lt__(self, other):\n",
        "        return self.negative_distance < other.negative_distance\n",
        "\n",
        "def find_k_nearest_patches_to_prototypes(dataloader, prototype_network_parallel, k=5, full_save=False, # save all the images\n",
        "                                         root_dir_for_saving_images='./nearest', prototype_activation_function_in_numpy=None):\n",
        "    prototype_network_parallel.eval()\n",
        "\n",
        "    log('find nearest patches', \"analysislog.txt\")\n",
        "    start = time.time()\n",
        "    n_prototypes = prototype_network_parallel.module.num_prototypes\n",
        "    \n",
        "    prototype_shape = prototype_network_parallel.module.prototype_shape\n",
        "    max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
        "\n",
        "    protoL_rf_info = prototype_network_parallel.module.proto_layer_rf_info\n",
        "\n",
        "    heaps = []\n",
        "    for _ in range(n_prototypes):\n",
        "        heaps.append([])\n",
        "\n",
        "    for idx, (search_batch_input, search_y) in enumerate(dataloader):\n",
        "        print('batch {}'.format(idx))\n",
        "        search_batch = search_batch_input\n",
        "        with torch.no_grad():\n",
        "            search_batch = search_batch.cuda()\n",
        "            proto_dist_torch = prototype_network_parallel.module.push_forward(search_batch)\n",
        "\n",
        "        proto_dist = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
        "\n",
        "        for img_idx, distance_map in enumerate(proto_dist):\n",
        "            for j in range(n_prototypes):\n",
        "                closest_patch_distance_to_prototype_j = np.amin(distance_map[j])\n",
        "\n",
        "                if full_save:\n",
        "                    closest_patch_indices_in_distance_map_j = \\\n",
        "                        list(np.unravel_index(np.argmin(distance_map[j],axis=None),\n",
        "                                              distance_map[j].shape))\n",
        "                    closest_patch_indices_in_distance_map_j = [0] + closest_patch_indices_in_distance_map_j\n",
        "                    closest_patch_indices_in_img = \\\n",
        "                        compute_rf_prototype(search_batch.size(2),\n",
        "                                             closest_patch_indices_in_distance_map_j,\n",
        "                                             protoL_rf_info)\n",
        "                    closest_patch = \\\n",
        "                        search_batch_input[img_idx, :,\n",
        "                                           closest_patch_indices_in_img[1]:closest_patch_indices_in_img[2],\n",
        "                                           closest_patch_indices_in_img[3]:closest_patch_indices_in_img[4]]\n",
        "                    closest_patch = closest_patch.numpy()\n",
        "                    closest_patch = np.transpose(closest_patch, (1, 2, 0))\n",
        "\n",
        "                    original_img = search_batch_input[img_idx].numpy()\n",
        "                    original_img = np.transpose(original_img, (1, 2, 0))\n",
        "\n",
        "                    if prototype_network_parallel.module.prototype_activation_function == 'log':\n",
        "                        act_pattern = np.log((distance_map[j] + 1)/(distance_map[j] + prototype_network_parallel.module.epsilon))\n",
        "                    elif prototype_network_parallel.module.prototype_activation_function == 'linear':\n",
        "                        act_pattern = max_dist - distance_map[j]\n",
        "                    else:\n",
        "                        act_pattern = prototype_activation_function_in_numpy(distance_map[j])\n",
        "\n",
        "                    patch_indices = closest_patch_indices_in_img[1:5]\n",
        "\n",
        "                    closest_patch = ImagePatch(patch=closest_patch,\n",
        "                                               label=search_y[img_idx],\n",
        "                                               distance=closest_patch_distance_to_prototype_j,\n",
        "                                               original_img=original_img,\n",
        "                                               act_pattern=act_pattern,\n",
        "                                               patch_indices=patch_indices)\n",
        "                else:\n",
        "                    closest_patch = ImagePatchInfo(label=search_y[img_idx],\n",
        "                                                   distance=closest_patch_distance_to_prototype_j)\n",
        "\n",
        "                if len(heaps[j]) < k:\n",
        "                    heapq.heappush(heaps[j], closest_patch)\n",
        "                else:\n",
        "                    heapq.heappushpop(heaps[j], closest_patch)\n",
        "\n",
        "    for j in range(n_prototypes):\n",
        "        heaps[j].sort()\n",
        "        heaps[j] = heaps[j][::-1]\n",
        "\n",
        "        if full_save:\n",
        "\n",
        "            dir_for_saving_images = os.path.join(root_dir_for_saving_images,\n",
        "                                                 str(j))\n",
        "            makedir(dir_for_saving_images)\n",
        "\n",
        "            labels = []\n",
        "\n",
        "            for i, patch in enumerate(heaps[j]):\n",
        "                # save the activation pattern of the original image where the patch comes from\n",
        "                np.save(os.path.join(dir_for_saving_images,\n",
        "                                     'nearest-' + str(i+1) + '_act.npy'),\n",
        "                        patch.act_pattern)\n",
        "                \n",
        "                # save the original image where the patch comes from\n",
        "                plt.imsave(fname=os.path.join(dir_for_saving_images,\n",
        "                                              'nearest-' + str(i+1) + '_original.png'),\n",
        "                           arr=patch.original_img,\n",
        "                           vmin=0.0,\n",
        "                           vmax=1.0)\n",
        "                \n",
        "                # overlay (upsampled) activation on original image and save the result\n",
        "                img_size = patch.original_img.shape[0]\n",
        "                upsampled_act_pattern = cv2.resize(patch.act_pattern,\n",
        "                                                   dsize=(img_size, img_size),\n",
        "                                                   interpolation=cv2.INTER_CUBIC)\n",
        "                rescaled_act_pattern = upsampled_act_pattern - np.amin(upsampled_act_pattern)\n",
        "                rescaled_act_pattern = rescaled_act_pattern / np.amax(rescaled_act_pattern)\n",
        "                heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_pattern), cv2.COLORMAP_JET)\n",
        "                heatmap = np.float32(heatmap) / 255\n",
        "                heatmap = heatmap[...,::-1]\n",
        "                overlayed_original_img = 0.5 * patch.original_img + 0.3 * heatmap\n",
        "                plt.imsave(fname=os.path.join(dir_for_saving_images,\n",
        "                                              'nearest-' + str(i+1) + '_original_with_heatmap.png'),\n",
        "                           arr=overlayed_original_img,\n",
        "                           vmin=0.0,\n",
        "                           vmax=1.0)\n",
        "                \n",
        "                # if different from original image, save the patch (i.e. receptive field)\n",
        "                if patch.patch.shape[0] != img_size or patch.patch.shape[1] != img_size:\n",
        "                    np.save(os.path.join(dir_for_saving_images,\n",
        "                                         'nearest-' + str(i+1) + '_receptive_field_indices.npy'),\n",
        "                            patch.patch_indices)\n",
        "                    plt.imsave(fname=os.path.join(dir_for_saving_images,\n",
        "                                              'nearest-' + str(i+1) + '_receptive_field.png'),\n",
        "                               arr=patch.patch,\n",
        "                               vmin=0.0,\n",
        "                               vmax=1.0)\n",
        "                    # save the receptive field patch with heatmap\n",
        "                    overlayed_patch = overlayed_original_img[patch.patch_indices[0]:patch.patch_indices[1],\n",
        "                                                             patch.patch_indices[2]:patch.patch_indices[3], :]\n",
        "                    plt.imsave(fname=os.path.join(dir_for_saving_images,\n",
        "                                              'nearest-' + str(i+1) + '_receptive_field_with_heatmap.png'),\n",
        "                               arr=overlayed_patch,\n",
        "                               vmin=0.0,\n",
        "                               vmax=1.0)\n",
        "                    \n",
        "                # save the highly activated patch    \n",
        "                high_act_patch_indices = find_high_activation_crop(upsampled_act_pattern)\n",
        "                high_act_patch = patch.original_img[high_act_patch_indices[0]:high_act_patch_indices[1],\n",
        "                                                    high_act_patch_indices[2]:high_act_patch_indices[3], :]\n",
        "                np.save(os.path.join(dir_for_saving_images,\n",
        "                                     'nearest-' + str(i+1) + '_high_act_patch_indices.npy'),\n",
        "                        high_act_patch_indices)\n",
        "                plt.imsave(fname=os.path.join(dir_for_saving_images,\n",
        "                                              'nearest-' + str(i+1) + '_high_act_patch.png'),\n",
        "                           arr=high_act_patch,\n",
        "                           vmin=0.0,\n",
        "                           vmax=1.0)\n",
        "                # save the original image with bounding box showing high activation patch\n",
        "                imsave_with_bbox(fname=os.path.join(dir_for_saving_images,\n",
        "                                       'nearest-' + str(i+1) + '_high_act_patch_in_original_img.png'),\n",
        "                                 img_rgb=patch.original_img,\n",
        "                                 bbox_height_start=high_act_patch_indices[0],\n",
        "                                 bbox_height_end=high_act_patch_indices[1],\n",
        "                                 bbox_width_start=high_act_patch_indices[2],\n",
        "                                 bbox_width_end=high_act_patch_indices[3], color=(0, 255, 255))\n",
        "            \n",
        "            labels = np.array([patch.label for patch in heaps[j]])\n",
        "            np.save(os.path.join(dir_for_saving_images, 'class_id.npy'),\n",
        "                    labels)\n",
        "\n",
        "\n",
        "    labels_all_prototype = np.array([[patch.label for patch in heaps[j]] for j in range(n_prototypes)])\n",
        "\n",
        "    if full_save:\n",
        "        np.save(os.path.join(root_dir_for_saving_images, 'full_class_id.npy'),\n",
        "                labels_all_prototype)\n",
        "\n",
        "    end = time.time()\n",
        "    log('\\tfind nearest patches time: \\t{0}'.format(end - start), \"analysislog.txt\")\n",
        "\n",
        "    return labels_all_prototype\n",
        "\n",
        "\n",
        "# receptive field\n",
        "def compute_rf_protoL_at_spatial_location(img_size, height_index, width_index, protoL_rf_info):\n",
        "    j, r, start = protoL_rf_info[1], protoL_rf_info[2], protoL_rf_info[3]\n",
        "    center_h, center_w = start + (height_index*j), start + (width_index*j)\n",
        "    return [max(int(center_h - (r/2)), 0), min(int(center_h + (r/2)), img_size),\n",
        "            max(int(center_w - (r/2)), 0), min(int(center_w + (r/2)), img_size)]\n",
        "\n",
        "def compute_rf_prototype(img_size, prototype_patch_index, protoL_rf_info):\n",
        "    img_index, height_index, width_index = prototype_patch_index[0], prototype_patch_index[1], prototype_patch_index[2]\n",
        "    rf_indices = compute_rf_protoL_at_spatial_location(img_size, height_index, width_index, protoL_rf_info)\n",
        "    return [img_index, rf_indices[0], rf_indices[1],\n",
        "            rf_indices[2], rf_indices[3]]\n",
        "\n",
        "\n",
        "def compute_layer_rf_info(layer_filter_size, layer_stride, layer_padding, previous_layer_rf_info):\n",
        "    n_in = previous_layer_rf_info[0] # input size\n",
        "    j_in = previous_layer_rf_info[1] # receptive field jump of input layer\n",
        "    r_in = previous_layer_rf_info[2] # receptive field size of input layer\n",
        "    start_in = previous_layer_rf_info[3] # center of receptive field of input layer\n",
        "\n",
        "    if layer_padding == 'SAME':\n",
        "        n_out = math.ceil(float(n_in) / float(layer_stride))\n",
        "        if (n_in % layer_stride == 0):\n",
        "            pad = max(layer_filter_size - layer_stride, 0)\n",
        "        else:\n",
        "            pad = max(layer_filter_size - (n_in % layer_stride), 0)\n",
        "    elif layer_padding == 'VALID':\n",
        "        pad = 0\n",
        "        n_out = math.ceil(float(n_in - layer_filter_size + 1) / float(layer_stride))\n",
        "    else:\n",
        "        pad = layer_padding * 2\n",
        "        n_out = math.floor((n_in - layer_filter_size + pad)/layer_stride) + 1\n",
        "\n",
        "    pL = math.floor(pad/2)\n",
        "\n",
        "    j_out = j_in * layer_stride\n",
        "    r_out = r_in + (layer_filter_size - 1)*j_in\n",
        "    start_out = start_in + ((layer_filter_size - 1)/2 - pL)*j_in\n",
        "    return [n_out, j_out, r_out, start_out]\n",
        "\n",
        "def compute_proto_layer_rf_info_v2(img_size, layer_filter_sizes, layer_strides, layer_paddings, prototype_kernel_size):\n",
        "    rf_info = [img_size, 1, 1, 0.5]\n",
        "    for i in range(len(layer_filter_sizes)):\n",
        "        filter_size, stride_size, padding_size = layer_filter_sizes[i], layer_strides[i], layer_paddings[i]\n",
        "        rf_info = compute_layer_rf_info(layer_filter_size=filter_size, layer_stride=stride_size,\n",
        "                                        layer_padding=padding_size, previous_layer_rf_info=rf_info)\n",
        "    proto_layer_rf_info = compute_layer_rf_info(layer_filter_size=prototype_kernel_size, layer_stride=1,\n",
        "                                                layer_padding='VALID', previous_layer_rf_info=rf_info)\n",
        "    return proto_layer_rf_info\n",
        "\n",
        "def push_prototypes(dataloader, # unn\n",
        "                    prototype_network_parallel, # nn\n",
        "                    preprocess_input_function=None, # normalize?\n",
        "                    prototype_layer_stride=1,\n",
        "                    root_dir_for_saving_prototypes=None,\n",
        "                    epoch_number=None,\n",
        "                    prototype_img_filename_prefix=None,\n",
        "                    prototype_self_act_filename_prefix=None,\n",
        "                    proto_bound_boxes_filename_prefix=None,\n",
        "                    save_prototype_class_identity=True,\n",
        "                    prototype_activation_function_in_numpy=None):\n",
        "\n",
        "    prototype_network_parallel.eval()\n",
        "    log('\\tpush', \"trainlog.txt\")\n",
        "\n",
        "    start = time.time()\n",
        "    prototype_shape = prototype_network_parallel.module.prototype_shape\n",
        "    n_prototypes = prototype_network_parallel.module.num_prototypes\n",
        "    global_min_proto_dist = np.full(n_prototypes, np.inf)\n",
        "    global_min_fmap_patches = np.zeros([n_prototypes, prototype_shape[1],prototype_shape[2],prototype_shape[3]])\n",
        "\n",
        "    if save_prototype_class_identity:\n",
        "        proto_rf_boxes = np.full(shape=[n_prototypes, 6], fill_value=-1)\n",
        "        proto_bound_boxes = np.full(shape=[n_prototypes, 6], fill_value=-1)\n",
        "    else:\n",
        "        proto_rf_boxes = np.full(shape=[n_prototypes, 5], fill_value=-1)\n",
        "        proto_bound_boxes = np.full(shape=[n_prototypes, 5], fill_value=-1)\n",
        "\n",
        "    if root_dir_for_saving_prototypes != None:\n",
        "        if epoch_number != None:\n",
        "            proto_epoch_dir = os.path.join(root_dir_for_saving_prototypes,\n",
        "                                           'epoch-'+str(epoch_number))\n",
        "            makedir(proto_epoch_dir)\n",
        "        else:\n",
        "            proto_epoch_dir = root_dir_for_saving_prototypes\n",
        "    else:\n",
        "        proto_epoch_dir = None\n",
        "\n",
        "    search_batch_size = dataloader.batch_size\n",
        "    num_classes = prototype_network_parallel.module.num_classes\n",
        "\n",
        "    for push_iter, (search_batch_input, search_y) in enumerate(dataloader):\n",
        "        start_index_of_search_batch = push_iter * search_batch_size\n",
        "        update_prototypes_on_batch(search_batch_input,\n",
        "                                   start_index_of_search_batch,\n",
        "                                   prototype_network_parallel,\n",
        "                                   global_min_proto_dist,\n",
        "                                   global_min_fmap_patches,\n",
        "                                   proto_rf_boxes,\n",
        "                                   proto_bound_boxes,\n",
        "                                   search_y=search_y,\n",
        "                                   num_classes=num_classes,\n",
        "                                   preprocess_input_function=preprocess_input_function,\n",
        "                                   prototype_layer_stride=prototype_layer_stride,\n",
        "                                   dir_for_saving_prototypes=proto_epoch_dir,\n",
        "                                   prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
        "                                   prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
        "                                   prototype_activation_function_in_numpy=prototype_activation_function_in_numpy)\n",
        "\n",
        "    if proto_epoch_dir != None and proto_bound_boxes_filename_prefix != None:\n",
        "        np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + '-receptive_field' + str(epoch_number) + '.npy'),\n",
        "                proto_rf_boxes)\n",
        "        np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + str(epoch_number) + '.npy'),\n",
        "                proto_bound_boxes)\n",
        "\n",
        "    log('\\tExecuting push ...', \"pushlog.txt\")\n",
        "    prototype_update = np.reshape(global_min_fmap_patches,\n",
        "                                  tuple(prototype_shape))\n",
        "    prototype_network_parallel.module.prototype_vectors.data.copy_(torch.tensor(prototype_update, dtype=torch.float32).cuda())\n",
        "    end = time.time()\n",
        "    log('\\tpush time: \\t{0}'.format(end -  start), \"pushlog.txt\")\n",
        "\n",
        "def update_prototypes_on_batch(search_batch_input,\n",
        "                               start_index_of_search_batch,\n",
        "                               prototype_network_parallel,\n",
        "                               global_min_proto_dist, # this will be updated\n",
        "                               global_min_fmap_patches, # this will be updated\n",
        "                               proto_rf_boxes, # this will be updated\n",
        "                               proto_bound_boxes, # this will be updated\n",
        "                               search_y=None, # required if class_specific == True\n",
        "                               num_classes=None, # required if class_specific == True\n",
        "                               preprocess_input_function=None,\n",
        "                               prototype_layer_stride=1,\n",
        "                               dir_for_saving_prototypes=None,\n",
        "                               prototype_img_filename_prefix=None,\n",
        "                               prototype_self_act_filename_prefix=None,\n",
        "                               prototype_activation_function_in_numpy=None):\n",
        "\n",
        "    prototype_network_parallel.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        search_batch = search_batch_input.cuda()\n",
        "        protoL_input_torch, proto_dist_torch = prototype_network_parallel.module.push_forward(search_batch)\n",
        "\n",
        "    protoL_input_ = np.copy(protoL_input_torch.detach().cpu().numpy())\n",
        "    proto_dist_ = np.copy(proto_dist_torch.detach().cpu().numpy())\n",
        "\n",
        "    del protoL_input_torch, proto_dist_torch\n",
        "\n",
        "    class_to_img_index_dict = {key: [] for key in range(num_classes)}\n",
        "        # img_y is the image's integer label\n",
        "    for img_index, img_y in enumerate(search_y):\n",
        "        img_label = img_y.item()\n",
        "        class_to_img_index_dict[img_label].append(img_index)\n",
        "\n",
        "    prototype_shape = prototype_network_parallel.module.prototype_shape\n",
        "    n_prototypes, proto_h, proto_w = prototype_shape[0], prototype_shape[2], prototype_shape[3]\n",
        "\n",
        "    for j in range(n_prototypes):\n",
        "        target_class = torch.argmax(prototype_network_parallel.module.prototype_class_identity[j]).item()\n",
        "        if len(class_to_img_index_dict[target_class]) == 0:\n",
        "            continue\n",
        "        proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]\n",
        "\n",
        "        batch_min_proto_dist_j = np.amin(proto_dist_j)\n",
        "        if batch_min_proto_dist_j < global_min_proto_dist[j]:\n",
        "            batch_argmin_proto_dist_j = \\\n",
        "                list(np.unravel_index(np.argmin(proto_dist_j, axis=None),\n",
        "                                      proto_dist_j.shape))\n",
        "            batch_argmin_proto_dist_j[0] = class_to_img_index_dict[target_class][batch_argmin_proto_dist_j[0]]\n",
        "\n",
        "            # retrieve the corresponding feature map patch\n",
        "            img_index_in_batch = batch_argmin_proto_dist_j[0]\n",
        "            fmap_height_start_index = batch_argmin_proto_dist_j[1] * prototype_layer_stride\n",
        "            fmap_height_end_index = fmap_height_start_index + proto_h\n",
        "            fmap_width_start_index = batch_argmin_proto_dist_j[2] * prototype_layer_stride\n",
        "            fmap_width_end_index = fmap_width_start_index + proto_w\n",
        "\n",
        "            batch_min_fmap_patch_j = protoL_input_[img_index_in_batch,\n",
        "                                                   :,\n",
        "                                                   fmap_height_start_index:fmap_height_end_index,\n",
        "                                                   fmap_width_start_index:fmap_width_end_index]\n",
        "\n",
        "            global_min_proto_dist[j] = batch_min_proto_dist_j\n",
        "            global_min_fmap_patches[j] = batch_min_fmap_patch_j\n",
        "            \n",
        "            # get the receptive field boundary of the image patch\n",
        "            # that generates the representation\n",
        "            protoL_rf_info = prototype_network_parallel.module.proto_layer_rf_info\n",
        "            rf_prototype_j = compute_rf_prototype(search_batch.size(2), batch_argmin_proto_dist_j, protoL_rf_info)\n",
        "            \n",
        "            # get the whole image\n",
        "            original_img_j = search_batch_input[rf_prototype_j[0]]\n",
        "            original_img_j = original_img_j.numpy()\n",
        "            original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
        "            original_img_size = original_img_j.shape[0]\n",
        "            \n",
        "            # crop out the receptive field\n",
        "            rf_img_j = original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
        "                                      rf_prototype_j[3]:rf_prototype_j[4],]\n",
        "            \n",
        "            # save the prototype receptive field information\n",
        "            proto_rf_boxes[j, 0] = rf_prototype_j[0] + start_index_of_search_batch\n",
        "            proto_rf_boxes[j, 1] = rf_prototype_j[1]\n",
        "            proto_rf_boxes[j, 2] = rf_prototype_j[2]\n",
        "            proto_rf_boxes[j, 3] = rf_prototype_j[3]\n",
        "            proto_rf_boxes[j, 4] = rf_prototype_j[4]\n",
        "            if proto_rf_boxes.shape[1] == 6 and search_y is not None:\n",
        "                proto_rf_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
        "\n",
        "            # find the highly activated region of the original image\n",
        "            proto_dist_img_j = proto_dist_[img_index_in_batch, j, :, :]\n",
        "            proto_act_img_j = np.log((proto_dist_img_j + 1) / (proto_dist_img_j + prototype_network_parallel.module.epsilon))\n",
        "            upsampled_act_img_j = cv2.resize(proto_act_img_j, dsize=(original_img_size, original_img_size), interpolation=cv2.INTER_CUBIC)\n",
        "            proto_bound_j = find_high_activation_crop(upsampled_act_img_j)\n",
        "            # crop out the image patch with high activation as prototype image\n",
        "            proto_img_j = original_img_j[proto_bound_j[0]:proto_bound_j[1], proto_bound_j[2]:proto_bound_j[3], :]\n",
        "\n",
        "            # save the prototype boundary (rectangular boundary of highly activated region)\n",
        "            proto_bound_boxes[j, 0] = proto_rf_boxes[j, 0]\n",
        "            proto_bound_boxes[j, 1] = proto_bound_j[0]\n",
        "            proto_bound_boxes[j, 2] = proto_bound_j[1]\n",
        "            proto_bound_boxes[j, 3] = proto_bound_j[2]\n",
        "            proto_bound_boxes[j, 4] = proto_bound_j[3]\n",
        "            if proto_bound_boxes.shape[1] == 6 and search_y is not None:\n",
        "                proto_bound_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
        "\n",
        "            if dir_for_saving_prototypes is not None:\n",
        "                if prototype_self_act_filename_prefix is not None:\n",
        "                    # save the numpy array of the prototype self activation\n",
        "                    np.save(os.path.join(dir_for_saving_prototypes,\n",
        "                                         prototype_self_act_filename_prefix + str(j) + '.npy'),\n",
        "                            proto_act_img_j)\n",
        "                if prototype_img_filename_prefix is not None:\n",
        "                    # save the whole image containing the prototype as png\n",
        "                    plt.imsave(os.path.join(dir_for_saving_prototypes, prototype_img_filename_prefix + '-original' + str(j) + '.png'),\n",
        "                               original_img_j, vmin=0.0, vmax=1.0)\n",
        "                    # overlay (upsampled) self activation on original image and save the result\n",
        "                    rescaled_act_img_j = upsampled_act_img_j - np.amin(upsampled_act_img_j)\n",
        "                    rescaled_act_img_j = rescaled_act_img_j / np.amax(rescaled_act_img_j)\n",
        "                    heatmap = cv2.applyColorMap(np.uint8(255*rescaled_act_img_j), cv2.COLORMAP_JET)\n",
        "                    heatmap = np.float32(heatmap) / 255\n",
        "                    heatmap = heatmap[...,::-1]\n",
        "                    overlayed_original_img_j = 0.5 * original_img_j + 0.3 * heatmap\n",
        "                    plt.imsave(os.path.join(dir_for_saving_prototypes, prototype_img_filename_prefix + '-original_with_self_act' + str(j) + '.png'),\n",
        "                               overlayed_original_img_j, vmin=0.0, vmax=1.0)\n",
        "                    \n",
        "                    # if different from the original (whole) image, save the prototype receptive field as png\n",
        "                    if rf_img_j.shape[0] != original_img_size or rf_img_j.shape[1] != original_img_size:\n",
        "                        plt.imsave(os.path.join(dir_for_saving_prototypes, prototype_img_filename_prefix + '-receptive_field' + str(j) + '.png'),\n",
        "                                   rf_img_j, vmin=0.0, vmax=1.0)\n",
        "                        overlayed_rf_img_j = overlayed_original_img_j[rf_prototype_j[1]:rf_prototype_j[2], rf_prototype_j[3]:rf_prototype_j[4]]\n",
        "                        plt.imsave(os.path.join(dir_for_saving_prototypes,prototype_img_filename_prefix + '-receptive_field_with_self_act' + str(j) + '.png'),\n",
        "                                   overlayed_rf_img_j, vmin=0.0, vmax=1.0)\n",
        "                    \n",
        "                    # save the prototype image (highly activated region of the whole image)\n",
        "                    plt.imsave(os.path.join(dir_for_saving_prototypes, prototype_img_filename_prefix + str(j) + '.png'),\n",
        "                               proto_img_j, vmin=0.0, vmax=1.0)            \n",
        "    del class_to_img_index_dict\n",
        "\n",
        "# resnet\n",
        "model_urls = {\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'\n",
        "}\n",
        "\n",
        "cfg = {'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "       'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "       'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "       'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],}\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    # class attribute\n",
        "    expansion = 1\n",
        "    num_layers = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # only conv with possibly not 1 stride\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # if stride is not 1 then self.downsample cannot be None\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # the residual connection\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        block_kernel_sizes = [3, 3]\n",
        "        block_strides = [self.stride, 1]\n",
        "        block_paddings = [1, 1]\n",
        "\n",
        "        return block_kernel_sizes, block_strides, block_paddings\n",
        "\n",
        "class ResNet_features(nn.Module):\n",
        "    '''\n",
        "    the convolutional layers of ResNet\n",
        "    the average pooling and final fully convolutional layer is removed\n",
        "    '''\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet_features, self).__init__()\n",
        "\n",
        "        self.inplanes = 64\n",
        "\n",
        "        # the first convolutional layer before the structured sequence of blocks\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # comes from the first conv and the following max pool\n",
        "        self.kernel_sizes = [7, 3]\n",
        "        self.strides = [2, 2]\n",
        "        self.paddings = [3, 1]\n",
        "\n",
        "        # the following layers, each layer is a sequence of blocks\n",
        "        self.block = block\n",
        "        self.layers = layers\n",
        "        self.layer1 = self._make_layer(block=block, planes=64, num_blocks=self.layers[0])\n",
        "        self.layer2 = self._make_layer(block=block, planes=128, num_blocks=self.layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block=block, planes=256, num_blocks=self.layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block=block, planes=512, num_blocks=self.layers[3], stride=2)\n",
        "\n",
        "        # initialize the parameters\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        # only the first block has downsample that is possibly not None\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        # keep track of every block's conv size, stride size, and padding size\n",
        "        for each_block in layers:\n",
        "            block_kernel_sizes, block_strides, block_paddings = each_block.block_conv_info()\n",
        "            self.kernel_sizes.extend(block_kernel_sizes)\n",
        "            self.strides.extend(block_strides)\n",
        "            self.paddings.extend(block_paddings)\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def conv_info(self):\n",
        "        return self.kernel_sizes, self.strides, self.paddings\n",
        "\n",
        "    def num_layers(self):\n",
        "        '''\n",
        "        the number of conv layers in the network, not counting the number\n",
        "        of bypass layers\n",
        "        '''\n",
        "\n",
        "        return (self.block.num_layers * self.layers[0]\n",
        "              + self.block.num_layers * self.layers[1]\n",
        "              + self.block.num_layers * self.layers[2]\n",
        "              + self.block.num_layers * self.layers[3]\n",
        "              + 1)\n",
        "\n",
        "def resnet34_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet34'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "base_architecture_to_features = {'resnet34': resnet34_features,}\n",
        "\n",
        "# model fugle\n",
        "class PPNet(nn.Module):\n",
        "\n",
        "    def __init__(self, features, img_size, prototype_shape,\n",
        "                 proto_layer_rf_info, num_classes, init_weights=True,\n",
        "                 prototype_activation_function='log',\n",
        "                 add_on_layers_type='bottleneck'):\n",
        "\n",
        "        super(PPNet, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.prototype_shape = prototype_shape # 2000, 512, 1, 1\n",
        "        self.num_prototypes = prototype_shape[0]\n",
        "        self.num_classes = num_classes\n",
        "        self.epsilon = 1e-4\n",
        "        self.prototype_activation_function = prototype_activation_function\n",
        "        self.prototype_class_identity = torch.zeros(self.num_prototypes, self.num_classes)\n",
        "\n",
        "        num_prototypes_per_class = self.num_prototypes // self.num_classes\n",
        "        for j in range(self.num_prototypes):\n",
        "            self.prototype_class_identity[j, j // num_prototypes_per_class] = 1\n",
        "\n",
        "        self.proto_layer_rf_info = proto_layer_rf_info\n",
        "        self.features = features\n",
        "\n",
        "        features_name = str(self.features).upper()\n",
        "        if features_name.startswith('VGG') or features_name.startswith('RES'):\n",
        "            first_add_on_layer_in_channels = [i for i in features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
        "        elif features_name.startswith('DENSE'):\n",
        "            first_add_on_layer_in_channels = [i for i in features.modules() if isinstance(i, nn.BatchNorm2d)][-1].num_features\n",
        "        else:\n",
        "            raise Exception('other base base_architecture NOT implemented')\n",
        "\n",
        "        if add_on_layers_type == 'bottleneck':\n",
        "            add_on_layers, current_in_channels = [], first_add_on_layer_in_channels\n",
        "            while (current_in_channels > self.prototype_shape[1]) or (len(add_on_layers) == 0):\n",
        "                current_out_channels = max(self.prototype_shape[1], (current_in_channels // 2))\n",
        "                add_on_layers.append(nn.Conv2d(in_channels=current_in_channels,\n",
        "                                               out_channels=current_out_channels, kernel_size=1))\n",
        "                add_on_layers.append(nn.ReLU())\n",
        "                add_on_layers.append(nn.Conv2d(in_channels=current_out_channels,\n",
        "                                               out_channels=current_out_channels,\n",
        "                                               kernel_size=1))\n",
        "                if current_out_channels > self.prototype_shape[1]:\n",
        "                    add_on_layers.append(nn.ReLU())\n",
        "                else:\n",
        "                    add_on_layers.append(nn.Sigmoid())\n",
        "                current_in_channels = current_in_channels // 2\n",
        "            self.add_on_layers = nn.Sequential(*add_on_layers)\n",
        "        else:\n",
        "            self.add_on_layers = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "                )\n",
        "        self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape), requires_grad=True)\n",
        "        self.ones = nn.Parameter(torch.ones(self.prototype_shape), requires_grad=False)\n",
        "        self.last_layer = nn.Linear(self.num_prototypes, self.num_classes, bias=False)\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def conv_features(self, x): #z\n",
        "        return self.add_on_layers(self.features(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def _weighted_l2_convolution(input, filter, weights):\n",
        "        # input of shape N * c * h * w torch.Size([40, 512, 7, 7])\n",
        "        # filter of shape P * c * h1 * w1\n",
        "        # weight of shape P * c * h1 * w1\n",
        "        input_patch_weighted_norm2 = F.conv2d(input=input ** 2, weight=weights)\n",
        "        filter_weighted_norm2_reshape = torch.sum(weights * filter ** 2, dim=(1, 2, 3)).view(-1, 1, 1)\n",
        "        weighted_inner_product = F.conv2d(input=input, weight=filter * weights)\n",
        "        intermediate_result = - 2 * weighted_inner_product + filter_weighted_norm2_reshape\n",
        "        distances = F.relu(input_patch_weighted_norm2 + intermediate_result)\n",
        "        return distances\n",
        "\n",
        "    def _l2_convolution(self, x): # x = z (torch.Size([40, 512, 7, 7]))\n",
        "        x2_patch_sum = F.conv2d(input=x ** 2, weight=self.ones)  # torch.Size([40, 2000, 7, 7])\n",
        "        xp = F.conv2d(input=x, weight=self.prototype_vectors) # torch.Size([40, 2000, 7, 7])\n",
        "        intermediate_result = - 2 * xp + torch.sum(self.prototype_vectors ** 2, dim=(1, 2, 3)).view(-1, 1, 1)\n",
        "        distances = F.relu(x2_patch_sum + intermediate_result)\n",
        "        return distances # torch.Size([40, 2000, 7, 7])\n",
        "\n",
        "    def prototype_distances(self, x): # dist(z, prototypes)\n",
        "        return self._l2_convolution(self.conv_features(x)) \n",
        "\n",
        "    def distance_2_similarity(self, distances):\n",
        "        return torch.log((distances + 1) / (distances + self.epsilon))\n",
        "\n",
        "    def forward(self, x):\n",
        "        distances = self.prototype_distances(x)\n",
        "        min_distances = -F.max_pool2d(-distances, kernel_size=(distances.size()[2], distances.size()[3]))\n",
        "        # print(min_distances.size()) torch.Size([40, 2000, 1, 1])\n",
        "        min_distances = min_distances.view(-1, self.num_prototypes)\n",
        "        # print(min_distances.size()) torch.Size([40, 2000])\n",
        "        prototype_activations = self.distance_2_similarity(min_distances)\n",
        "        logits = self.last_layer(prototype_activations)\n",
        "        return logits, min_distances\n",
        "\n",
        "    def push_forward(self, x):\n",
        "        conv_output = self.conv_features(x)\n",
        "        distances = self._l2_convolution(conv_output)\n",
        "        return conv_output, distances\n",
        "\n",
        "    def prune_prototypes(self, prototypes_to_prune):\n",
        "        prototypes_to_keep = list(set(range(self.num_prototypes)) - set(prototypes_to_prune))\n",
        "        self.prototype_vectors = nn.Parameter(self.prototype_vectors.data[prototypes_to_keep, ...], requires_grad=True)\n",
        "        self.prototype_shape = list(self.prototype_vectors.size())\n",
        "        self.num_prototypes = self.prototype_shape[0]\n",
        "        self.last_layer.in_features = self.num_prototypes\n",
        "        self.last_layer.out_features = self.num_classes\n",
        "        self.last_layer.weight.data = self.last_layer.weight.data[:, prototypes_to_keep]\n",
        "        self.ones = nn.Parameter(self.ones.data[prototypes_to_keep, ...], requires_grad=False)\n",
        "        self.prototype_class_identity = self.prototype_class_identity[prototypes_to_keep,:]\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.add_on_layers.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "def initialize_model(model_name = \"\"):\n",
        "    features = base_architecture_to_features['resnet34'](pretrained = True)\n",
        "    layer_filter_sizes, layer_strides, layer_paddings = features.conv_info()\n",
        "\n",
        "    ppnet = PPNet(features=features,\n",
        "                  img_size = img_size,\n",
        "                  prototype_shape = (2000, 512, 1, 1),\n",
        "                  num_classes = 200,\n",
        "                  init_weights = True,\n",
        "                  prototype_activation_function = 'log',\n",
        "                  add_on_layers_type = 'bottleneck',\n",
        "                  proto_layer_rf_info = compute_proto_layer_rf_info_v2(img_size = 224,\n",
        "                                                                      layer_filter_sizes = layer_filter_sizes,\n",
        "                                                                      layer_strides = layer_strides,\n",
        "                                                                      layer_paddings = layer_paddings,\n",
        "                                                                      prototype_kernel_size = 1))\n",
        "    if model_name != \"\":\n",
        "        checkpoint = torch.load(model_dir + model_name)\n",
        "        ppnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    ppnet = ppnet.cuda()\n",
        "    ppnet_multi = torch.nn.DataParallel(ppnet)\n",
        "    return ppnet, ppnet_multi\n",
        "\n",
        "def _train_or_test(model, dataloader, optimizer=None, use_l1_mask=True,\n",
        "                   coefs=None):\n",
        "    is_train = optimizer is not None\n",
        "    start = time.time()\n",
        "    n_examples, n_correct, n_batches, total_cross_entropy = 0, 0, 0, 0\n",
        "    total_cluster_cost, total_separation_cost, total_avg_separation_cost = 0, 0, 0\n",
        "    \n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for _, (image, label) in enumerate(dataloader):\n",
        "        input, target = image.cuda(), label.cuda()\n",
        "\n",
        "        grad_req = torch.enable_grad() if is_train else torch.no_grad()\n",
        "        with grad_req:\n",
        "            output, min_distances = model(input)\n",
        "\n",
        "            # compute loss\n",
        "            cross_entropy = torch.nn.functional.cross_entropy(output, target)\n",
        "            max_dist = (model.module.prototype_shape[1] * model.module.prototype_shape[2] * model.module.prototype_shape[3])\n",
        "            prototypes_of_correct_class = torch.t(model.module.prototype_class_identity[:,label]).cuda()\n",
        "            inverted_distances, _ = torch.max((max_dist - min_distances) * prototypes_of_correct_class, dim=1)\n",
        "            cluster_cost = torch.mean(max_dist - inverted_distances)\n",
        "\n",
        "            # calculate separation cost\n",
        "            prototypes_of_wrong_class = 1 - prototypes_of_correct_class\n",
        "            inverted_distances_to_nontarget_prototypes, _ = \\\n",
        "                torch.max((max_dist - min_distances) * prototypes_of_wrong_class, dim=1)\n",
        "            separation_cost = torch.mean(max_dist - inverted_distances_to_nontarget_prototypes)\n",
        "\n",
        "            # calculate avg cluster cost\n",
        "            avg_separation_cost = \\\n",
        "                torch.sum(min_distances * prototypes_of_wrong_class, dim=1) / torch.sum(prototypes_of_wrong_class, dim=1)\n",
        "            avg_separation_cost = torch.mean(avg_separation_cost)\n",
        "                \n",
        "            if use_l1_mask:\n",
        "                l1_mask = 1 - torch.t(model.module.prototype_class_identity).cuda()\n",
        "                l1 = (model.module.last_layer.weight * l1_mask).norm(p=1)\n",
        "            else:\n",
        "                l1 = model.module.last_layer.weight.norm(p=1) \n",
        "\n",
        "            # evaluation statistics\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            n_examples += target.size(0)\n",
        "            n_correct += (predicted == target).sum().item()\n",
        "            n_batches += 1\n",
        "            total_cross_entropy += cross_entropy.item()\n",
        "            total_cluster_cost += cluster_cost.item()\n",
        "            total_separation_cost += separation_cost.item()\n",
        "            total_avg_separation_cost += avg_separation_cost.item()\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        if is_train:\n",
        "            del prototypes_of_correct_class\n",
        "            del l1_mask\n",
        "            with torch.cuda.amp.autocast():\n",
        "                if coefs is not None:\n",
        "                    loss = (coefs['crs_ent'] * cross_entropy + coefs['clst'] * cluster_cost\n",
        "                            + coefs['sep'] * separation_cost + coefs['l1'] * l1)\n",
        "                else:\n",
        "                    loss = cross_entropy + 0.8 * cluster_cost - 0.08 * separation_cost + 1e-4 * l1\n",
        "            optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "    end = time.time()\n",
        "\n",
        "    log('\\ttime: \\t{0}'.format(end -  start), \"trainlog.txt\")\n",
        "    log('\\tcross ent: \\t{0}'.format(total_cross_entropy / n_batches), \"trainlog.txt\")\n",
        "    log('\\tcluster: \\t{0}'.format(total_cluster_cost / n_batches), \"trainlog.txt\")\n",
        "    log('\\tseparation:\\t{0}'.format(total_separation_cost / n_batches), \"trainlog.txt\")\n",
        "    log('\\tavg separation:\\t{0}'.format(total_avg_separation_cost / n_batches), \"trainlog.txt\")\n",
        "    log('\\taccu: \\t\\t{0}%'.format(n_correct / n_examples * 100), \"trainlog.txt\")\n",
        "    log('\\tl1: \\t\\t{0}'.format(model.module.last_layer.weight.norm(p=1).item()), \"trainlog.txt\")\n",
        "    p = model.module.prototype_vectors.view(model.module.num_prototypes, -1).cpu()\n",
        "    with torch.no_grad():\n",
        "        p_avg_pair_dist = torch.mean(list_of_distances(p, p))\n",
        "    log('\\tp dist pair: \\t{0}'.format(p_avg_pair_dist.item()), \"trainlog.txt\")\n",
        "\n",
        "    return n_correct / n_examples\n",
        "\n",
        "def train(model, dataloader, optimizer, coefs=None):\n",
        "    log('\\ttrain', \"trainlog.txt\")\n",
        "    model.train()\n",
        "    return _train_or_test(model=model, dataloader=dataloader, optimizer=optimizer, coefs=coefs)\n",
        "  \n",
        "def test(model, dataloader):\n",
        "    log('\\ttest', \"trainlog.txt\")\n",
        "    model.eval()\n",
        "    return _train_or_test(model=model, dataloader=dataloader, optimizer=None)\n",
        "\n",
        "def last_only(model):\n",
        "    for p in model.module.features.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.module.add_on_layers.parameters():\n",
        "        p.requires_grad = False\n",
        "    model.module.prototype_vectors.requires_grad = False\n",
        "    for p in model.module.last_layer.parameters():\n",
        "        p.requires_grad = True\n",
        "    log('\\tlast layer', \"trainlog.txt\")\n",
        "\n",
        "def warm_only(model):\n",
        "    for p in model.module.features.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.module.add_on_layers.parameters():\n",
        "        p.requires_grad = True\n",
        "    model.module.prototype_vectors.requires_grad = True\n",
        "    for p in model.module.last_layer.parameters():\n",
        "        p.requires_grad = True\n",
        "    log('\\twarm', \"trainlog.txt\")\n",
        "\n",
        "def joint(model):\n",
        "    for p in model.module.features.parameters():\n",
        "        p.requires_grad = True\n",
        "    for p in model.module.add_on_layers.parameters():\n",
        "        p.requires_grad = True\n",
        "    model.module.prototype_vectors.requires_grad = True\n",
        "    for p in model.module.last_layer.parameters():\n",
        "        p.requires_grad = True\n",
        "    log('\\tjoint', \"trainlog.txt\")\n",
        "\n",
        "def fit(model, modelmulti, epochs, warm_epochs, epoch_reached):\n",
        "    # log('start training', \"trainlog.txt\")\n",
        "    # for epoch in range(epoch_reached, epochs):\n",
        "    #     model.train()\n",
        "    #     modelmulti.train()\n",
        "    #     log('epoch: \\t{0}'.format(epoch), \"trainlog.txt\")\n",
        "        \n",
        "    #     if epoch < warm_epochs:\n",
        "    #         warm_only(model=modelmulti)\n",
        "    #         train(model=modelmulti, dataloader=train_loader, optimizer=warm_optimizer, coefs=coefs)\n",
        "    #     else:\n",
        "    #         joint(model=modelmulti)\n",
        "    #         train(model=modelmulti, dataloader=train_loader, optimizer=joint_optimizer, coefs=coefs)\n",
        "    #         joint_lr_scheduler.step()\n",
        "    #     model.eval()\n",
        "    #     modelmulti.eval()\n",
        "    #     accu = test(model=modelmulti, dataloader=test_loader)\n",
        "    #     torch.save({\n",
        "    #         'epoch': epoch,\n",
        "    #         'model_state_dict': model.state_dict(),\n",
        "    #         'joint_optimizer_state_dict': joint_optimizer.state_dict(),\n",
        "    #         'joint_lr_scheduler_state_dict': joint_lr_scheduler.state_dict(),\n",
        "    #         'last_layer_optimizer_state_dict': last_layer_optimizer.state_dict(),\n",
        "    #         'warm_optimizer_state_dict' : warm_optimizer.state_dict()\n",
        "    #         }, os.path.join(model_dir, (str(epoch) + 'nopush' + '{0:.4f}.pth').format(accu)))\n",
        "\n",
        "        if True:#epoch >= push_start and epoch in push_epochs:\n",
        "            push_prototypes(\n",
        "                train_push_loader, # pytorch dataloader unnorm\n",
        "                prototype_network_parallel=modelmulti,\n",
        "                preprocess_input_function = preprocess, # norma?\n",
        "                prototype_layer_stride=1,\n",
        "                root_dir_for_saving_prototypes = model_dir + '/img/',\n",
        "                epoch_number = 15, #epoch, # if not provided, prototypes saved previously will be overwritten\n",
        "                prototype_img_filename_prefix = 'prototype-img',\n",
        "                prototype_self_act_filename_prefix = 'prototype-self-act',\n",
        "                proto_bound_boxes_filename_prefix = 'bb',\n",
        "                save_prototype_class_identity=True)\n",
        "            # last_only(model=modelmulti)\n",
        "            # for i in range(100):\n",
        "            #     log('iteration: \\t{0}'.format(i), \"trainlog.txt\")\n",
        "            #     _ = train(model=modelmulti, dataloader=train_loader, optimizer=last_layer_optimizer, coefs=coefs)\n",
        "            #     accu = test(model=modelmulti, dataloader=test_loader)\n",
        "            #     torch.save({\n",
        "            #         'epoch': epoch,\n",
        "            #         'model_state_dict': model.state_dict(),\n",
        "            #         'joint_optimizer_state_dict': joint_optimizer.state_dict(),\n",
        "            #         'joint_lr_scheduler_state_dict': joint_lr_scheduler.state_dict(),\n",
        "            #         'last_layer_optimizer_state_dict': last_layer_optimizer.state_dict(),\n",
        "            #         'warm_optimizer_state_dict' : warm_optimizer.state_dict()\n",
        "            #         }, os.path.join(model_dir, (str(epoch) + '_' + str(i) + 'push' + '{0:.4f}.pth').format(accu)))\n",
        "\n",
        "################################################################################################################################\n",
        "\n",
        "cuda = torch.device('cuda')  if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using : \", cuda)\n",
        "\n",
        "epochs = 1000\n",
        "warm_epochs = 5\n",
        "epoch_reached = 0\n",
        "push_start = 10\n",
        "push_epochs = [i for i in range(epochs) if i % 10 == 0]\n",
        "coefs = {'crs_ent': 1, 'clst': 0.8, 'sep': -0.08, 'l1': 1e-4,}\n",
        "\n",
        "img_size = 224\n",
        "prototype_shape = (1000, 128, 1, 1)\n",
        "num_classes = 100\n",
        "\n",
        "train_batch_size = 80\n",
        "test_batch_size = 100\n",
        "train_push_batch_size = 75\n",
        "\n",
        "# sets\n",
        "train_dataset = datasets.ImageFolder(\n",
        "        path + 'datasets/cub200_cropped/train_cropped_augmented/', \n",
        "        transforms.Compose([transforms.Resize(size=(img_size, img_size)), transforms.ToTensor(), \n",
        "                            transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))]))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4, pin_memory=True) # 4 workers?\n",
        "\n",
        "train_push_dataset = datasets.ImageFolder(\n",
        "        path + 'datasets/cub200_cropped/train_cropped/',\n",
        "        transforms.Compose([transforms.Resize(size=(img_size, img_size)),transforms.ToTensor()]))\n",
        "train_push_loader = torch.utils.data.DataLoader(\n",
        "        train_push_dataset, batch_size=train_push_batch_size, shuffle=False, num_workers=4, pin_memory=True) # 4 workers?\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "        path + 'datasets/cub200_cropped/test_cropped/',\n",
        "        transforms.Compose([transforms.Resize(size=(img_size, img_size)),transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),]))\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4, pin_memory=True) # 4 workers?\n",
        "\n",
        "################################################################################################################################\n",
        "################################################################################################################################\n",
        "################################################################################################################################\n",
        "\n",
        "model_name = \"10push0.5407.pth\"\n",
        "if model_name != \"\":\n",
        "    checkpoint = torch.load(model_dir + model_name)\n",
        "ppnet, ppnet_multi = initialize_model(model_name = model_name)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# optimizers \n",
        "joint_optimizer = torch.optim.Adam([{'params': ppnet.features.parameters(), 'lr': 1e-4, 'weight_decay': 1e-3}, \n",
        "                                        {'params': ppnet.add_on_layers.parameters(), 'lr': 3e-3, 'weight_decay': 1e-3},\n",
        "                                        {'params': ppnet.prototype_vectors, 'lr': 3e-3},])\n",
        "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size = 5, gamma=0.1)\n",
        "last_layer_optimizer = torch.optim.Adam([{'params': ppnet.last_layer.parameters(), 'lr': 1e-4}])\n",
        "warm_optimizer = torch.optim.Adam([{'params': ppnet.add_on_layers.parameters(), 'lr': 3e-3, 'weight_decay': 1e-3},\n",
        "                                {'params': ppnet.prototype_vectors, 'lr': 3e-3},])\n",
        "\n",
        "if model_name != \"\":\n",
        "    epoch_reached = checkpoint['epoch'] + 1 # next epoch\n",
        "    joint_optimizer.load_state_dict(checkpoint['joint_optimizer_state_dict'])\n",
        "    joint_lr_scheduler.load_state_dict(checkpoint['joint_lr_scheduler_state_dict'])\n",
        "    last_layer_optimizer.load_state_dict(checkpoint['last_layer_optimizer_state_dict'])\n",
        "    warm_optimizer.load_state_dict(checkpoint['warm_optimizer_state_dict'])\n",
        "    \n",
        "\n",
        "# run fitting\n",
        "fit(ppnet, ppnet_multi, epochs = epochs, warm_epochs = warm_epochs, epoch_reached = epoch_reached)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prune\n",
        "def prune_prototypes(dataloader,\n",
        "                     prototype_network_parallel,\n",
        "                     k,\n",
        "                     prune_threshold,\n",
        "                     preprocess_input_function,\n",
        "                     original_model_dir,\n",
        "                     epoch_number,\n",
        "                     #model_name=None,\n",
        "                     copy_prototype_imgs=True):\n",
        "    ### run global analysis\n",
        "    nearest_train_patch_class_ids = \\\n",
        "        find_k_nearest_patches_to_prototypes(dataloader=dataloader,\n",
        "                                             prototype_network_parallel=prototype_network_parallel,\n",
        "                                             k=k,\n",
        "                                             preprocess_input_function=preprocess_input_function,\n",
        "                                             full_save=False)\n",
        "\n",
        "    ### find prototypes to prune\n",
        "    original_num_prototypes = prototype_network_parallel.module.num_prototypes\n",
        "    \n",
        "    prototypes_to_prune = []\n",
        "    for j in range(prototype_network_parallel.module.num_prototypes):\n",
        "        class_j = torch.argmax(prototype_network_parallel.module.prototype_class_identity[j]).item()\n",
        "        nearest_train_patch_class_counts_j = Counter(nearest_train_patch_class_ids[j])\n",
        "        # if no such element is in Counter, it will return 0\n",
        "        if nearest_train_patch_class_counts_j[class_j] < prune_threshold:\n",
        "            prototypes_to_prune.append(j)\n",
        "\n",
        "    log('k = {}, prune_threshold = {}'.format(k, prune_threshold))\n",
        "    log('{} prototypes will be pruned'.format(len(prototypes_to_prune)))\n",
        "\n",
        "    ### bookkeeping of prototypes to be pruned\n",
        "    class_of_prototypes_to_prune = \\\n",
        "        torch.argmax(\n",
        "            prototype_network_parallel.module.prototype_class_identity[prototypes_to_prune],\n",
        "            dim=1).numpy().reshape(-1, 1)\n",
        "    prototypes_to_prune_np = np.array(prototypes_to_prune).reshape(-1, 1)\n",
        "    prune_info = np.hstack((prototypes_to_prune_np, class_of_prototypes_to_prune))\n",
        "    makedir(os.path.join(original_model_dir, 'pruned_prototypes_epoch{}_k{}_pt{}'.format(epoch_number,\n",
        "                                          k,\n",
        "                                          prune_threshold)))\n",
        "    np.save(os.path.join(original_model_dir, 'pruned_prototypes_epoch{}_k{}_pt{}'.format(epoch_number,\n",
        "                                          k,\n",
        "                                          prune_threshold), 'prune_info.npy'),\n",
        "            prune_info)\n",
        "\n",
        "    ### prune prototypes\n",
        "    prototype_network_parallel.module.prune_prototypes(prototypes_to_prune)\n",
        "    #torch.save(obj=prototype_network_parallel.module,\n",
        "    #           f=os.path.join(original_model_dir, 'pruned_prototypes_epoch{}_k{}_pt{}'.format(epoch_number,\n",
        "    #                                              k,\n",
        "    #                                              prune_threshold),\n",
        "    #                          model_name + '-pruned.pth'))\n",
        "    if copy_prototype_imgs:\n",
        "        original_img_dir = os.path.join(original_model_dir, 'img', 'epoch-%d' % epoch_number)\n",
        "        dst_img_dir = os.path.join(original_model_dir,\n",
        "                                   'pruned_prototypes_epoch{}_k{}_pt{}'.format(epoch_number,\n",
        "                                                                               k,\n",
        "                                                                               prune_threshold),\n",
        "                                   'img', 'epoch-%d' % epoch_number)\n",
        "        makedir(dst_img_dir)\n",
        "        prototypes_to_keep = list(set(range(original_num_prototypes)) - set(prototypes_to_prune))\n",
        "        \n",
        "        for idx in range(len(prototypes_to_keep)):\n",
        "            shutil.copyfile(src=os.path.join(original_img_dir, 'prototype-img%d.png' % prototypes_to_keep[idx]),\n",
        "                            dst=os.path.join(dst_img_dir, 'prototype-img%d.png' % idx))\n",
        "            \n",
        "            shutil.copyfile(src=os.path.join(original_img_dir, 'prototype-img-original%d.png' % prototypes_to_keep[idx]),\n",
        "                            dst=os.path.join(dst_img_dir, 'prototype-img-original%d.png' % idx))\n",
        "            \n",
        "            shutil.copyfile(src=os.path.join(original_img_dir, 'prototype-img-original_with_self_act%d.png' % prototypes_to_keep[idx]),\n",
        "                            dst=os.path.join(dst_img_dir, 'prototype-img-original_with_self_act%d.png' % idx))\n",
        "            \n",
        "            shutil.copyfile(src=os.path.join(original_img_dir, 'prototype-self-act%d.npy' % prototypes_to_keep[idx]),\n",
        "                            dst=os.path.join(dst_img_dir, 'prototype-self-act%d.npy' % idx))\n",
        "\n",
        "\n",
        "            bb = np.load(os.path.join(original_img_dir, 'bb%d.npy' % epoch_number))\n",
        "            bb = bb[prototypes_to_keep]\n",
        "            np.save(os.path.join(dst_img_dir, 'bb%d.npy' % epoch_number),\n",
        "                    bb)\n",
        "\n",
        "            bb_rf = np.load(os.path.join(original_img_dir, 'bb-receptive_field%d.npy' % epoch_number))\n",
        "            bb_rf = bb_rf[prototypes_to_keep]\n",
        "            np.save(os.path.join(dst_img_dir, 'bb-receptive_field%d.npy' % epoch_number),\n",
        "                    bb_rf)\n",
        "    \n",
        "    return prune_info\n",
        "\n",
        "\n",
        "optimize_last_layer = True\n",
        "\n",
        "# pruning parameters\n",
        "k = 6\n",
        "prune_threshold = 3\n",
        "\n",
        "original_model_dir = args.modeldir[0] #'./saved_models/densenet161/003/'\n",
        "original_model_name = args.model[0] #'10_16push0.8007.pth'\n",
        "\n",
        "need_push = ('nopush' in original_model_name)\n",
        "if need_push:\n",
        "    assert(False) # pruning must happen after push\n",
        "else:\n",
        "    epoch = original_model_name.split('push')[0]\n",
        "\n",
        "if '_' in epoch:\n",
        "    epoch = int(epoch.split('_')[0])\n",
        "else:\n",
        "    epoch = int(epoch)\n",
        "\n",
        "model_dir = os.path.join(original_model_dir, 'pruned_prototypes_epoch{}_k{}_pt{}'.format(epoch,\n",
        "                                          k,\n",
        "                                          prune_threshold))\n",
        "makedir(model_dir)\n",
        "shutil.copy(src=os.path.join(os.getcwd(), __file__), dst=model_dir)\n",
        "\n",
        "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'prune.log'))\n",
        "\n",
        "ppnet = torch.load(original_model_dir + original_model_name)\n",
        "ppnet = ppnet.cuda()\n",
        "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
        "class_specific = True\n",
        "\n",
        "# load the data\n",
        "from settings import train_dir, test_dir, train_push_dir\n",
        "\n",
        "train_batch_size = 80\n",
        "test_batch_size = 100\n",
        "img_size = 224\n",
        "train_push_batch_size = 80\n",
        "\n",
        "normalize = transforms.Normalize(mean=mean,\n",
        "                                 std=std)\n",
        "\n",
        "# train set\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "    num_workers=4, pin_memory=False)\n",
        "\n",
        "# test set\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    test_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]))\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
        "    num_workers=4, pin_memory=False)\n",
        "\n",
        "log('training set size: {0}'.format(len(train_loader.dataset)))\n",
        "log('test set size: {0}'.format(len(test_loader.dataset)))\n",
        "log('batch size: {0}'.format(train_batch_size))\n",
        "\n",
        "# push set: needed for pruning because it is unnormalized\n",
        "train_push_dataset = datasets.ImageFolder(\n",
        "    train_push_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=(img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "train_push_loader = torch.utils.data.DataLoader(\n",
        "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
        "    num_workers=4, pin_memory=False)\n",
        "    \n",
        "log('push set size: {0}'.format(len(train_push_loader.dataset)))\n",
        "\n",
        "tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
        "         class_specific=class_specific, log=log)\n",
        "\n",
        "# prune prototypes\n",
        "log('prune')\n",
        "prune_prototypes(dataloader=train_push_loader,\n",
        "                 prototype_network_parallel=ppnet_multi,\n",
        "                 k=k,\n",
        "                 prune_threshold=prune_threshold,\n",
        "                 preprocess_input_function=preprocess_input_function, # normalize\n",
        "                 original_model_dir=original_model_dir,\n",
        "                 epoch_number=epoch,\n",
        "                 #model_name=None,\n",
        "                 copy_prototype_imgs=True)\n",
        "accu = test(model=ppnet_multi, dataloader=test_loader,\n",
        "            class_specific=class_specific, log=log)\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'joint_optimizer_state_dict': joint_optimizer.state_dict(),\n",
        "    'joint_lr_scheduler_state_dict': joint_lr_scheduler.state_dict(),\n",
        "    'last_layer_optimizer_state_dict': last_layer_optimizer.state_dict(),\n",
        "    'warm_optimizer_state_dict' : warm_optimizer.state_dict()\n",
        "    }, os.path.join(model_dir, (str(epoch) + 'push' + '{0:.4f}.pth').format(accu)))\n",
        "\n",
        "# last layer optimization\n",
        "if optimize_last_layer:\n",
        "    last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': 1e-4}]\n",
        "    last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
        "\n",
        "    coefs = {\n",
        "        'crs_ent': 1,\n",
        "        'clst': 0.8,\n",
        "        'sep': -0.08,\n",
        "        'l1': 1e-4,\n",
        "    }\n",
        "\n",
        "    log('optimize last layer')\n",
        "    last_only(model=ppnet_multi, log=log)\n",
        "    for i in range(100):\n",
        "        log('iteration: \\t{0}'.format(i))\n",
        "        _ = train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n",
        "                      class_specific=class_specific, coefs=coefs, log=log)\n",
        "        accu = test(model=ppnet_multi, dataloader=test_loader,\n",
        "                        class_specific=class_specific, log=log)\n",
        "        save_model_w_condition(model=ppnet, model_dir=model_dir,\n",
        "                               model_name=original_model_name.split('push')[0] + '_' + str(i) + 'prune',\n",
        "                               accu=accu,\n",
        "                               target_accu=0.70, log=log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "local_analysis/10push0/\n",
            "image index 0 in batch\n",
            "[1080 1081 1082 1083 1084 1085 1086 1087 1088 1089]\n",
            "[430 431 432 433 434 435 436 437 438 439]\n",
            "[110 111 112 113 114 115 116 117 118 119]\n",
            "[1840 1841 1842 1843 1844 1845 1846 1847 1848 1849]\n",
            "[310 311 312 313 314 315 316 317 318 319]\n"
          ]
        }
      ],
      "source": [
        "# local\n",
        "model_name = \"10push0.7409.pth\"\n",
        "model_dir = path + 'pretrained_models/'\n",
        "checkpoint = torch.load(model_dir + model_name)\n",
        "\n",
        "prototype_info_name = \"bb10.npy\"\n",
        "test_image_path = path + \"local_analysis/_test/\" + \"012.Yellow_headed_Blackbird_original_Yellow_Headed_Blackbird_0003_8337.jpg_0d48c69f-4c15-4cb0-8192-014296bcc000.jpg\"\n",
        "test_image_label = 15\n",
        "save_analysis_path = \"local_analysis/\" + model_name.split(\".\")[0] + \"/\"\n",
        "print(save_analysis_path)\n",
        "start_epoch_number = 10 #int(re.search(r'\\d+', model_name).group(0))\n",
        "makedir(save_analysis_path)\n",
        "makedir(path + \"local_analysis/_test\")\n",
        "load_img_dir = path + \"/pretrained_models/img\"\n",
        "\n",
        "ppnet, ppnet_multi = initialize_model(model_name)\n",
        "max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
        "prototype_info = np.load(path + '/pretrained_models/img/epoch-10/' + prototype_info_name)\n",
        "prototype_img_identity = prototype_info[:, -1]\n",
        "\n",
        "# confirm prototype connects most strongly to its own class\n",
        "prototype_max_connection = torch.argmax(ppnet.last_layer.weight, dim=0).cpu().numpy()\n",
        "log('model : ' + model_name, save_analysis_path + 'local_analysis_log.txt')\n",
        "log('Prototypes are chosen from ' + str(len(set(prototype_img_identity))) + ' number of classes.', save_analysis_path + 'local_analysis_log.txt')\n",
        "log('Their class identities are: ' + str(prototype_img_identity), save_analysis_path + 'local_analysis_log.txt')\n",
        "if np.sum(prototype_max_connection == prototype_img_identity) == ppnet.num_prototypes:\n",
        "    log('All prototypes connect most strongly to their respective classes.', save_analysis_path + 'local_analysis_log.txt')\n",
        "else:\n",
        "    log('WARNING: Not all prototypes connect most strongly to their respective classes.', save_analysis_path + 'local_analysis_log.txt')\n",
        "\n",
        "def save_preprocessed_img(fname, preprocessed_imgs, index=0):\n",
        "    print('image index {0} in batch'.format(index))\n",
        "    img_copy = copy.deepcopy(preprocessed_imgs[index:index+1])\n",
        "    undo_preprocessed_img = np.transpose(undo_preprocess(img_copy)[0].detach().cpu().numpy(), [1,2,0])\n",
        "    plt.imsave(fname, undo_preprocessed_img)\n",
        "    return undo_preprocessed_img\n",
        "\n",
        "def save_prototype(fname, epoch, index):\n",
        "    p_img = plt.imread(os.path.join(load_img_dir, 'epoch-'+str(epoch), 'prototype-img'+str(index)+'.png'))\n",
        "    plt.imsave(fname, p_img)\n",
        "    \n",
        "def save_prototype_self_activation(fname, epoch, index):\n",
        "    p_img = plt.imread(os.path.join(load_img_dir, 'epoch-'+str(epoch), 'prototype-img-original_with_self_act'+str(index)+'.png'))\n",
        "    plt.imsave(fname, p_img)\n",
        "\n",
        "def save_prototype_original_img_with_bbox(fname, epoch, index, bbox_height_start, bbox_height_end, bbox_width_start, bbox_width_end):\n",
        "    p_img_bgr = cv2.imread(os.path.join(load_img_dir, 'epoch-'+str(epoch), 'prototype-img-original'+str(index)+'.png'))\n",
        "    cv2.rectangle(p_img_bgr, (bbox_width_start, bbox_height_start), (bbox_width_end-1, bbox_height_end-1), (0, 255, 255), thickness=2)\n",
        "    plt.imsave(fname, np.float32(p_img_bgr[...,::-1]) / 255)\n",
        "\n",
        "def imsave_with_bbox(fname, img_rgb, bbox_height_start, bbox_height_end, bbox_width_start, bbox_width_end):\n",
        "    img_bgr_uint8 = cv2.cvtColor(np.uint8(255*img_rgb), cv2.COLOR_RGB2BGR)\n",
        "    cv2.rectangle(img_bgr_uint8, (bbox_width_start, bbox_height_start), (bbox_width_end-1, bbox_height_end-1), (0, 255, 255), thickness=2)\n",
        "    plt.imsave(fname, np.float32(img_bgr_uint8[...,::-1]) / 255)\n",
        "\n",
        "# load the test image and forward it through the network\n",
        "preprocess = transforms.Compose([transforms.Resize((img_size,img_size)), transforms.ToTensor(), \n",
        "             transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))])\n",
        "\n",
        "images_test, labels_test = Variable(preprocess(Image.open(test_image_path)).unsqueeze(0)).cuda(), torch.tensor([test_image_label])\n",
        "\n",
        "logits, min_distances = ppnet_multi(images_test)\n",
        "conv_output, distances = ppnet.push_forward(images_test)\n",
        "prototype_activations = ppnet.distance_2_similarity(min_distances)\n",
        "prototype_activation_patterns = ppnet.distance_2_similarity(distances)\n",
        "\n",
        "tables = []\n",
        "for i in range(logits.size(0)):\n",
        "    tables.append((torch.argmax(logits, dim=1)[i].item(), labels_test[i].item()))\n",
        "    log(str(i) + ' ' + str(tables[-1]), save_analysis_path + 'local_analysis_log.txt')\n",
        "\n",
        "predicted_cls, correct_cls = tables[0][0], tables[0][1]\n",
        "log('Predicted: ' + str(predicted_cls), save_analysis_path + 'local_analysis_log.txt')\n",
        "log('Actual: ' + str(correct_cls), save_analysis_path + 'local_analysis_log.txt')\n",
        "original_img = save_preprocessed_img(save_analysis_path + 'original_img.png', images_test, 0)\n",
        "\n",
        "##### MOST ACTIVATED (NEAREST) 10 PROTOTYPES OF THIS IMAGE\n",
        "makedir(save_analysis_path + 'most_activated_prototypes')\n",
        "\n",
        "log('Most activated 10 prototypes of this image:', save_analysis_path + 'local_analysis_log.txt')\n",
        "array_act, sorted_indices_act = torch.sort(prototype_activations[0])\n",
        "for i in range(1, 11):\n",
        "    log('top {0} activated prototype for this image:'.format(i), save_analysis_path + 'local_analysis_log.txt')\n",
        "    save_prototype(os.path.join(save_analysis_path, 'most_activated_prototypes','top-%d_activated_prototype.png' % i),\n",
        "                   start_epoch_number, sorted_indices_act[-i].item())\n",
        "    save_prototype_original_img_with_bbox(fname=os.path.join(save_analysis_path, 'most_activated_prototypes',\n",
        "                                                             'top-%d_activated_prototype_in_original_pimg.png' % i),\n",
        "                                          epoch=start_epoch_number,\n",
        "                                          index=sorted_indices_act[-i].item(),\n",
        "                                          bbox_height_start=prototype_info[sorted_indices_act[-i].item()][1],\n",
        "                                          bbox_height_end=prototype_info[sorted_indices_act[-i].item()][2],\n",
        "                                          bbox_width_start=prototype_info[sorted_indices_act[-i].item()][3],\n",
        "                                          bbox_width_end=prototype_info[sorted_indices_act[-i].item()][4])\n",
        "    save_prototype_self_activation(os.path.join(save_analysis_path, 'most_activated_prototypes', 'top-%d_activated_prototype_self_act.png' % i),\n",
        "                                   start_epoch_number, sorted_indices_act[-i].item())\n",
        "    log('prototype index: {0}'.format(sorted_indices_act[-i].item()), save_analysis_path + 'local_analysis_log.txt')\n",
        "    log('prototype class identity: {0}'.format(prototype_img_identity[sorted_indices_act[-i].item()]), save_analysis_path + 'local_analysis_log.txt')\n",
        "    if prototype_max_connection[sorted_indices_act[-i].item()] != prototype_img_identity[sorted_indices_act[-i].item()]:\n",
        "        log('prototype connection identity: {0}'.format(prototype_max_connection[sorted_indices_act[-i].item()]), save_analysis_path + 'local_analysis_log.txt')\n",
        "    log('activation value (similarity score): {0}'.format(array_act[-i]), save_analysis_path + 'local_analysis_log.txt')\n",
        "    log('last layer connection with predicted class: {0}'.format(ppnet.last_layer.weight[predicted_cls][sorted_indices_act[-i].item()]), save_analysis_path + 'local_analysis_log.txt')\n",
        "    \n",
        "    activation_pattern = prototype_activation_patterns[0][sorted_indices_act[-i].item()].detach().cpu().numpy()\n",
        "    upsampled_activation_pattern = cv2.resize(activation_pattern, dsize=(img_size, img_size),\n",
        "                                              interpolation=cv2.INTER_CUBIC)\n",
        "    \n",
        "    # show the most highly activated patch of the image by this prototype\n",
        "    high_act_patch_indices = find_high_activation_crop(upsampled_activation_pattern)\n",
        "    high_act_patch = original_img[high_act_patch_indices[0]:high_act_patch_indices[1],\n",
        "                                  high_act_patch_indices[2]:high_act_patch_indices[3], :]\n",
        "    log('most highly activated patch of the chosen image by this prototype:', save_analysis_path + 'local_analysis_log.txt')\n",
        "    plt.imsave(os.path.join(save_analysis_path, 'most_activated_prototypes',\n",
        "                            'most_highly_activated_patch_by_top-%d_prototype.png' % i),\n",
        "               high_act_patch)\n",
        "    log('most highly activated patch by this prototype shown in the original image:', save_analysis_path + 'local_analysis_log.txt')\n",
        "    imsave_with_bbox(fname=os.path.join(save_analysis_path, 'most_activated_prototypes',\n",
        "                            'most_highly_activated_patch_in_original_img_by_top-%d_prototype.png' % i),\n",
        "                     img_rgb=original_img,\n",
        "                     bbox_height_start=high_act_patch_indices[0],\n",
        "                     bbox_height_end=high_act_patch_indices[1],\n",
        "                     bbox_width_start=high_act_patch_indices[2],\n",
        "                     bbox_width_end=high_act_patch_indices[3])\n",
        "    \n",
        "    # show the image overlayed with prototype activation map\n",
        "    rescaled_activation_pattern = upsampled_activation_pattern - np.amin(upsampled_activation_pattern)\n",
        "    rescaled_activation_pattern = rescaled_activation_pattern / np.amax(rescaled_activation_pattern)\n",
        "    heatmap = np.float32(cv2.applyColorMap(np.uint8(255*rescaled_activation_pattern), cv2.COLORMAP_JET)) / 255\n",
        "    heatmap = heatmap[...,::-1]\n",
        "    overlayed_img = 0.5 * original_img + 0.3 * heatmap\n",
        "    log('prototype activation map of the chosen image:', save_analysis_path + 'local_analysis_log.txt')\n",
        "    #plt.axis('off')\n",
        "    plt.imsave(os.path.join(save_analysis_path, 'most_activated_prototypes',\n",
        "                            'prototype_activation_map_by_top-%d_prototype.png' % i),overlayed_img)\n",
        "    log('--------------------------------------------------------------', save_analysis_path + 'local_analysis_log.txt')\n",
        "\n",
        "##### PROTOTYPES FROM TOP-k CLASSES\n",
        "k = 5\n",
        "log('Prototypes from top-%d classes:' % k, save_analysis_path + 'local_analysis_log.txt')\n",
        "topk_logits, topk_classes = torch.topk(logits[0], k=k)\n",
        "for i,c in enumerate(topk_classes.detach().cpu().numpy()):\n",
        "    makedir(os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1)))\n",
        "\n",
        "    log('top %d predicted class: %d' % (i+1, c), save_analysis_path + 'local_analysis_log.txt')\n",
        "    log('logit of the class: %f' % topk_logits[i], save_analysis_path + 'local_analysis_log.txt')\n",
        "    class_prototype_indices = np.nonzero(ppnet.prototype_class_identity.detach().cpu().numpy()[:, c])[0]\n",
        "    print(class_prototype_indices)\n",
        "    class_prototype_activations = prototype_activations[0][class_prototype_indices]\n",
        "    _, sorted_indices_cls_act = torch.sort(class_prototype_activations)\n",
        "\n",
        "    prototype_cnt = 1\n",
        "    for j in reversed(sorted_indices_cls_act.detach().cpu().numpy()):\n",
        "        prototype_index = class_prototype_indices[j]\n",
        "        save_prototype(os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                       'top-%d_activated_prototype.png' % prototype_cnt),\n",
        "                       start_epoch_number, prototype_index)\n",
        "        save_prototype_original_img_with_bbox(fname=os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                                                                 'top-%d_activated_prototype_in_original_pimg.png' % prototype_cnt),\n",
        "                                              epoch=start_epoch_number,\n",
        "                                              index=prototype_index,\n",
        "                                              bbox_height_start=prototype_info[prototype_index][1],\n",
        "                                              bbox_height_end=prototype_info[prototype_index][2],\n",
        "                                              bbox_width_start=prototype_info[prototype_index][3],\n",
        "                                              bbox_width_end=prototype_info[prototype_index][4])\n",
        "        save_prototype_self_activation(os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                                                    'top-%d_activated_prototype_self_act.png' % prototype_cnt),\n",
        "                                       start_epoch_number, prototype_index)\n",
        "        log('prototype index: {0}'.format(prototype_index), save_analysis_path + 'local_analysis_log.txt')\n",
        "        log('prototype class identity: {0}'.format(prototype_img_identity[prototype_index]), save_analysis_path + 'local_analysis_log.txt')\n",
        "        if prototype_max_connection[prototype_index] != prototype_img_identity[prototype_index]:\n",
        "            log('prototype connection identity: {0}'.format(prototype_max_connection[prototype_index]), save_analysis_path + 'local_analysis_log.txt')\n",
        "        log('activation value (similarity score): {0}'.format(prototype_activations[0][prototype_index]), save_analysis_path + 'local_analysis_log.txt')\n",
        "        log('last layer connection: {0}'.format(ppnet.last_layer.weight[c][prototype_index]), save_analysis_path + 'local_analysis_log.txt')\n",
        "        \n",
        "        activation_pattern = prototype_activation_patterns[0][prototype_index].detach().cpu().numpy()\n",
        "        upsampled_activation_pattern = cv2.resize(activation_pattern, dsize=(img_size, img_size),\n",
        "                                                  interpolation=cv2.INTER_CUBIC)\n",
        "        \n",
        "        # show the most highly activated patch of the image by this prototype\n",
        "        high_act_patch_indices = find_high_activation_crop(upsampled_activation_pattern)\n",
        "        high_act_patch = original_img[high_act_patch_indices[0]:high_act_patch_indices[1],\n",
        "                                      high_act_patch_indices[2]:high_act_patch_indices[3], :]\n",
        "        log('most highly activated patch of the chosen image by this prototype:', save_analysis_path + 'local_analysis_log.txt')\n",
        "        #plt.axis('off')\n",
        "        plt.imsave(os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                                'most_highly_activated_patch_by_top-%d_prototype.png' % prototype_cnt),\n",
        "                   high_act_patch)\n",
        "        log('most highly activated patch by this prototype shown in the original image:', save_analysis_path + 'local_analysis_log.txt')\n",
        "        imsave_with_bbox(fname=os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                                            'most_highly_activated_patch_in_original_img_by_top-%d_prototype.png' % prototype_cnt),\n",
        "                         img_rgb=original_img,\n",
        "                         bbox_height_start=high_act_patch_indices[0],\n",
        "                         bbox_height_end=high_act_patch_indices[1],\n",
        "                         bbox_width_start=high_act_patch_indices[2],\n",
        "                         bbox_width_end=high_act_patch_indices[3])\n",
        "        \n",
        "        # show the image overlayed with prototype activation map\n",
        "        rescaled_activation_pattern = upsampled_activation_pattern - np.amin(upsampled_activation_pattern)\n",
        "        rescaled_activation_pattern = rescaled_activation_pattern / np.amax(rescaled_activation_pattern)\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255*rescaled_activation_pattern), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        heatmap = heatmap[...,::-1]\n",
        "        overlayed_img = 0.5 * original_img + 0.3 * heatmap\n",
        "        log('prototype activation map of the chosen image:', save_analysis_path + 'local_analysis_log.txt')\n",
        "        #plt.axis('off')\n",
        "        plt.imsave(os.path.join(save_analysis_path, 'top-%d_class_prototypes' % (i+1),\n",
        "                                'prototype_activation_map_by_top-%d_prototype.png' % prototype_cnt),\n",
        "                   overlayed_img)\n",
        "        log('--------------------------------------------------------------', save_analysis_path + 'local_analysis_log.txt')\n",
        "        prototype_cnt += 1\n",
        "    log('***************************************************************', save_analysis_path + 'local_analysis_log.txt')\n",
        "\n",
        "if predicted_cls == correct_cls:\n",
        "    log('Prediction is correct.', save_analysis_path + 'local_analysis_log.txt')\n",
        "else:\n",
        "    log('Prediction is wrong.', save_analysis_path + 'local_analysis_log.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# global analysis\n",
        "\n",
        "model_name = \"\"\n",
        "\n",
        "ppnet, ppnet_multi = initialize_model(model_name)\n",
        "\n",
        "batch_size, start_epoch_number = 100, 19\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    path + '/dataset/cub200_cropped/train_cropped/',\n",
        "    transforms.Compose([transforms.Resize(size=(img_size, img_size)),transforms.ToTensor(),]))\n",
        "    \n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                           num_workers=4, pin_memory=True)\n",
        "\n",
        "test_dataset = datasets.ImageFolder(\n",
        "    path + '/dataset/cub200_cropped/test_cropped/',\n",
        "    transforms.Compose([transforms.Resize(size=(img_size, img_size)), transforms.ToTensor(),]))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                          num_workers=4, pin_memory=True)\n",
        "\n",
        "root_dir_for_saving_train_images = os.path.join(path + \"/pretrained_models/\", model_name.split('.pth')[0] + '/_nearest_train/')\n",
        "root_dir_for_saving_test_images = os.path.join(path + \"/pretrained_models/\", model_name.split('.pth')[0] + '/_nearest_test/')\n",
        "makedir(root_dir_for_saving_train_images)\n",
        "makedir(root_dir_for_saving_test_images)\n",
        "\n",
        "\n",
        "# save prototypes in original images\n",
        "load_img_dir = os.path.join(path + \"pretrained_models/\", 'img')\n",
        "prototype_info = np.load(os.path.join(load_img_dir, 'epoch-'+str(start_epoch_number), 'bb'+str(start_epoch_number)+'.npy'))\n",
        "\n",
        "for j in range(ppnet.num_prototypes):\n",
        "    makedir(os.path.join(root_dir_for_saving_train_images, str(j)))\n",
        "    makedir(os.path.join(root_dir_for_saving_test_images, str(j)))\n",
        "    save_prototype_original_img_with_bbox(fname=os.path.join(root_dir_for_saving_train_images, str(j),'prototype_in_original_pimg.png'),\n",
        "                                          epoch=start_epoch_number, index=j, bbox_height_start=prototype_info[j][1], \n",
        "                                          bbox_height_end=prototype_info[j][2], bbox_width_start=prototype_info[j][3], \n",
        "                                          bbox_width_end=prototype_info[j][4])\n",
        "    save_prototype_original_img_with_bbox(fname=os.path.join(root_dir_for_saving_test_images, str(j), 'prototype_in_original_pimg.png'),\n",
        "                                          epoch=start_epoch_number, index=j, bbox_height_start=prototype_info[j][1],\n",
        "                                          bbox_height_end=prototype_info[j][2], bbox_width_start=prototype_info[j][3],\n",
        "                                          bbox_width_end=prototype_info[j][4])\n",
        "k = 5\n",
        "\n",
        "find_k_nearest_patches_to_prototypes(dataloader=train_loader, prototype_network_parallel=ppnet_multi, k=k+1, \n",
        "                                     full_save=True, root_dir_for_saving_images=root_dir_for_saving_train_images)\n",
        "\n",
        "find_k_nearest_patches_to_prototypes(dataloader=test_loader, prototype_network_parallel=ppnet_multi, k=k, \n",
        "                                     full_save=True, root_dir_for_saving_images=root_dir_for_saving_test_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"vgg19-dcbb9e9d.pth\"\n",
        "start_epoch_number = int(re.search(r'\\d+', model_name).group(0))\n",
        "base_architecture = model_name.split('-')[0]\n",
        "print(start_epoch_number)\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# srun -p image1 --cpus-per-task=5 --pty --time=02:00:00 bash\n",
        "# jupyter notebook --no-browser --port=3312\n",
        "# ssh -N -f -L localhost:8001:localhost:3312 cluster\n",
        "\n",
        "# srun -p gpu --pty --time=01:00:00 --gres=gpu:1 bash\n",
        "# jupyter-notebook --port=12100 --no-browser\n",
        "# ssh -N -L 12100:127.0.0.1:12100 cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kMEpZN70tSmK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to ./pretrained_models\\resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:05<00:00, 8.21MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet_features(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to ./pretrained_models\\resnet34-333f7ec4.pth\n",
            " 72%|███████▏  | 60.2M/83.3M [00:06<00:02, 9.40MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-6-32459b9b03e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr18_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[0mr34_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet34_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr34_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-32459b9b03e2>\u001b[0m in \u001b[0;36mresnet34_features\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mmy_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'resnet34'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[0mmy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fc.weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0mmy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fc.bias'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# r is Optional[Match[str]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[0mhash_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    472\u001b[0m                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[0;32m    473\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# base\n",
        "# base_architecture_to_features = {'resnet18': resnet18_features,\n",
        "#                                  'resnet34': resnet34_features,\n",
        "#                                  'resnet50': resnet50_features,\n",
        "#                                  'resnet101': resnet101_features,\n",
        "#                                  'resnet152': resnet152_features,\n",
        "#                                  'densenet121': densenet121_features,\n",
        "#                                  'densenet161': densenet161_features,\n",
        "#                                  'densenet169': densenet169_features,\n",
        "#                                  'densenet201': densenet201_features,\n",
        "#                                  'vgg11': vgg11_features,\n",
        "#                                  'vgg11_bn': vgg11_bn_features,\n",
        "#                                  'vgg13': vgg13_features,\n",
        "#                                  'vgg13_bn': vgg13_bn_features,\n",
        "#                                  'vgg16': vgg16_features,\n",
        "#                                  'vgg16_bn': vgg16_bn_features,\n",
        "#                                  'vgg19': vgg19_features,\n",
        "#                                  'vgg19_bn': vgg19_bn_features}\n",
        "\n",
        "# resnet\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "model_dir = './pretrained_models'\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    # class attribute\n",
        "    expansion = 1\n",
        "    num_layers = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # only conv with possibly not 1 stride\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        # if stride is not 1 then self.downsample cannot be None\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # the residual connection\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        block_kernel_sizes = [3, 3]\n",
        "        block_strides = [self.stride, 1]\n",
        "        block_paddings = [1, 1]\n",
        "\n",
        "        return block_kernel_sizes, block_strides, block_paddings\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # class attribute\n",
        "    expansion = 4\n",
        "    num_layers = 3\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        # only conv with possibly not 1 stride\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # if stride is not 1 then self.downsample cannot be None\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        block_kernel_sizes = [1, 3, 1]\n",
        "        block_strides = [1, self.stride, 1]\n",
        "        block_paddings = [0, 1, 0]\n",
        "\n",
        "        return block_kernel_sizes, block_strides, block_paddings\n",
        "\n",
        "\n",
        "class ResNet_features(nn.Module):\n",
        "    '''\n",
        "    the convolutional layers of ResNet\n",
        "    the average pooling and final fully convolutional layer is removed\n",
        "    '''\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet_features, self).__init__()\n",
        "\n",
        "        self.inplanes = 64\n",
        "\n",
        "        # the first convolutional layer before the structured sequence of blocks\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # comes from the first conv and the following max pool\n",
        "        self.kernel_sizes = [7, 3]\n",
        "        self.strides = [2, 2]\n",
        "        self.paddings = [3, 1]\n",
        "\n",
        "        # the following layers, each layer is a sequence of blocks\n",
        "        self.block = block\n",
        "        self.layers = layers\n",
        "        self.layer1 = self._make_layer(block=block, planes=64, num_blocks=self.layers[0])\n",
        "        self.layer2 = self._make_layer(block=block, planes=128, num_blocks=self.layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block=block, planes=256, num_blocks=self.layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block=block, planes=512, num_blocks=self.layers[3], stride=2)\n",
        "\n",
        "        # initialize the parameters\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        # only the first block has downsample that is possibly not None\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        # keep track of every block's conv size, stride size, and padding size\n",
        "        for each_block in layers:\n",
        "            block_kernel_sizes, block_strides, block_paddings = each_block.block_conv_info()\n",
        "            self.kernel_sizes.extend(block_kernel_sizes)\n",
        "            self.strides.extend(block_strides)\n",
        "            self.paddings.extend(block_paddings)\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def conv_info(self):\n",
        "        return self.kernel_sizes, self.strides, self.paddings\n",
        "\n",
        "    def num_layers(self):\n",
        "        '''\n",
        "        the number of conv layers in the network, not counting the number\n",
        "        of bypass layers\n",
        "        '''\n",
        "\n",
        "        return (self.block.num_layers * self.layers[0]\n",
        "              + self.block.num_layers * self.layers[1]\n",
        "              + self.block.num_layers * self.layers[2]\n",
        "              + self.block.num_layers * self.layers[3]\n",
        "              + 1)\n",
        "\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     template = 'resnet{}_features'\n",
        "    #     return template.format(self.num_layers() + 1)\n",
        "\n",
        "def resnet18_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet18'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet34'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet50'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet101'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152_features(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_features(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        my_dict = model_zoo.load_url(model_urls['resnet152'], model_dir=model_dir)\n",
        "        my_dict.pop('fc.weight')\n",
        "        my_dict.pop('fc.bias')\n",
        "        model.load_state_dict(my_dict, strict=False)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    r18_features = resnet18_features(pretrained=True)\n",
        "    print(r18_features)\n",
        "\n",
        "    r34_features = resnet34_features(pretrained=True)\n",
        "    print(r34_features)\n",
        "\n",
        "    r50_features = resnet50_features(pretrained=True)\n",
        "    print(r50_features)\n",
        "\n",
        "    r101_features = resnet101_features(pretrained=True)\n",
        "    print(r101_features)\n",
        "\n",
        "    r152_features = resnet152_features(pretrained=True)\n",
        "    print(r152_features)\n",
        "\n",
        "# densenet\n",
        "model_urls = {\n",
        "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
        "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
        "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
        "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
        "}\n",
        "model_dir = './pretrained_models'\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "\n",
        "    num_layers = 2\n",
        "\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                        growth_rate, kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "\n",
        "        # channelwise concatenation\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "    def layer_conv_info(self):\n",
        "        layer_kernel_sizes = [1, 3]\n",
        "        layer_strides = [1, 1]\n",
        "        layer_paddings = [0, 1]\n",
        "\n",
        "        return layer_kernel_sizes, layer_strides, layer_paddings\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        self.block_kernel_sizes = []\n",
        "        self.block_strides = []\n",
        "        self.block_paddings = []\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            layer_kernel_sizes, layer_strides, layer_paddings = layer.layer_conv_info()\n",
        "            self.block_kernel_sizes.extend(layer_kernel_sizes)\n",
        "            self.block_strides.extend(layer_strides)\n",
        "            self.block_paddings.extend(layer_paddings)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "        self.num_layers = _DenseLayer.num_layers * num_layers\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        return self.block_kernel_sizes, self.block_strides, self.block_paddings\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "\n",
        "    num_layers = 1\n",
        "\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2)) # AvgPool2d has no padding\n",
        "\n",
        "    def block_conv_info(self):\n",
        "        return [1, 2], [1, 2], [0, 0]\n",
        "\n",
        "\n",
        "class DenseNet_features(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
        "\n",
        "        super(DenseNet_features, self).__init__()\n",
        "        self.kernel_sizes = []\n",
        "        self.strides = []\n",
        "        self.paddings = []\n",
        "\n",
        "        self.n_layers = 0\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(in_channels=3, out_channels=num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        self.kernel_sizes.extend([7, 3])\n",
        "        self.strides.extend([2, 2])\n",
        "        self.paddings.extend([3, 1])\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            self.n_layers += block.num_layers\n",
        "\n",
        "            block_kernel_sizes, block_strides, block_paddings = block.block_conv_info()\n",
        "            self.kernel_sizes.extend(block_kernel_sizes)\n",
        "            self.strides.extend(block_strides)\n",
        "            self.paddings.extend(block_paddings)\n",
        "\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "\n",
        "                self.n_layers += trans.num_layers\n",
        "\n",
        "                block_kernel_sizes, block_strides, block_paddings = trans.block_conv_info()\n",
        "                self.kernel_sizes.extend(block_kernel_sizes)\n",
        "                self.strides.extend(block_strides)\n",
        "                self.paddings.extend(block_paddings)\n",
        "\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "        self.features.add_module('final_relu', nn.ReLU(inplace=True))\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def conv_info(self):\n",
        "        return self.kernel_sizes, self.strides, self.paddings\n",
        "\n",
        "    def num_layers(self):\n",
        "        return self.n_layers\n",
        "\n",
        "    # def __repr__(self):\n",
        "    #     template = 'densenet{}_features'\n",
        "    #     return template.format((self.num_layers() + 2))\n",
        "\n",
        "\n",
        "def densenet121_features(pretrained=False, **kwargs):\n",
        "    r\"\"\"Densenet-121 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = DenseNet_features(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                     **kwargs)\n",
        "    if pretrained:\n",
        "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
        "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "        # They are also in the checkpoints in model_urls. This pattern is used\n",
        "        # to find such keys.\n",
        "        pattern = re.compile(\n",
        "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "        state_dict = model_zoo.load_url(model_urls['densenet121'], model_dir=model_dir)\n",
        "        for key in list(state_dict.keys()):\n",
        "            '''\n",
        "            example\n",
        "            key 'features.denseblock4.denselayer24.norm.2.running_var'\n",
        "            res.group(1) 'features.denseblock4.denselayer24.norm'\n",
        "            res.group(2) '2.running_var'\n",
        "            new_key 'features.denseblock4.denselayer24.norm2.running_var'\n",
        "            '''\n",
        "            res = pattern.match(key)\n",
        "            if res:\n",
        "                new_key = res.group(1) + res.group(2)\n",
        "                state_dict[new_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "\n",
        "        del state_dict['classifier.weight']\n",
        "        del state_dict['classifier.bias']\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet169_features(pretrained=False, **kwargs):\n",
        "    r\"\"\"Densenet-169 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = DenseNet_features(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n",
        "                     **kwargs)\n",
        "    if pretrained:\n",
        "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
        "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "        # They are also in the checkpoints in model_urls. This pattern is used\n",
        "        # to find such keys.\n",
        "        pattern = re.compile(\n",
        "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "        state_dict = model_zoo.load_url(model_urls['densenet169'], model_dir=model_dir)\n",
        "        for key in list(state_dict.keys()):\n",
        "            '''\n",
        "            example\n",
        "            key 'features.denseblock4.denselayer24.norm.2.running_var'\n",
        "            res.group(1) 'features.denseblock4.denselayer24.norm'\n",
        "            res.group(2) '2.running_var'\n",
        "            new_key 'features.denseblock4.denselayer24.norm2.running_var'\n",
        "            '''\n",
        "            res = pattern.match(key)\n",
        "            if res:\n",
        "                new_key = res.group(1) + res.group(2)\n",
        "                state_dict[new_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "\n",
        "        del state_dict['classifier.weight']\n",
        "        del state_dict['classifier.bias']\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet201_features(pretrained=False, **kwargs):\n",
        "    r\"\"\"Densenet-201 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = DenseNet_features(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n",
        "                     **kwargs)\n",
        "    if pretrained:\n",
        "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
        "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "        # They are also in the checkpoints in model_urls. This pattern is used\n",
        "        # to find such keys.\n",
        "        pattern = re.compile(\n",
        "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "        state_dict = model_zoo.load_url(model_urls['densenet201'], model_dir=model_dir)\n",
        "        for key in list(state_dict.keys()):\n",
        "            '''\n",
        "            example\n",
        "            key 'features.denseblock4.denselayer24.norm.2.running_var'\n",
        "            res.group(1) 'features.denseblock4.denselayer24.norm'\n",
        "            res.group(2) '2.running_var'\n",
        "            new_key 'features.denseblock4.denselayer24.norm2.running_var'\n",
        "            '''\n",
        "            res = pattern.match(key)\n",
        "            if res:\n",
        "                new_key = res.group(1) + res.group(2)\n",
        "                state_dict[new_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "\n",
        "        del state_dict['classifier.weight']\n",
        "        del state_dict['classifier.bias']\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet161_features(pretrained=False, **kwargs):\n",
        "    model = DenseNet_features(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n",
        "                     **kwargs)\n",
        "    if pretrained:\n",
        "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
        "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "        # They are also in the checkpoints in model_urls. This pattern is used\n",
        "        # to find such keys.\n",
        "        pattern = re.compile(\n",
        "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "\n",
        "        state_dict = model_zoo.load_url(model_urls['densenet161'], model_dir=model_dir)\n",
        "        for key in list(state_dict.keys()):\n",
        "            '''\n",
        "            example\n",
        "            key 'features.denseblock4.denselayer24.norm.2.running_var'\n",
        "            res.group(1) 'features.denseblock4.denselayer24.norm'\n",
        "            res.group(2) '2.running_var'\n",
        "            new_key 'features.denseblock4.denselayer24.norm2.running_var'\n",
        "            '''\n",
        "            res = pattern.match(key)\n",
        "            if res:\n",
        "                new_key = res.group(1) + res.group(2)\n",
        "                state_dict[new_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "\n",
        "\n",
        "        del state_dict['classifier.weight']\n",
        "        del state_dict['classifier.bias']\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    d161 = densenet161_features(True)\n",
        "    print(d161)\n",
        "    d201 = densenet201_features(True)\n",
        "    print(d201)\n",
        "    d169 = densenet169_features(True)\n",
        "    print(d169)\n",
        "    d121 = densenet121_features(True)\n",
        "    print(d121)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ProtoPNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
