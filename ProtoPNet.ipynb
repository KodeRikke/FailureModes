{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProtoPNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# set path\n",
        "path = \"C:/Users/OhRai/Desktop/\" # path containing CUB_200_2011\n",
        "\n",
        "# libraries\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import Augmentor\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# functions\n",
        "def makedir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "metadata": {
        "id": "W8z4mEV5rw0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQWAWgLSi9qE"
      },
      "outputs": [],
      "source": [
        "# process data\n",
        "\n",
        "# raw data path\n",
        "makedir(path + 'ProtoPNet-master/datasets/cub200_cropped/test_cropped/')\n",
        "makedir(path + 'ProtoPNet-master/datasets/cub200_cropped/train_cropped/')\n",
        "\n",
        "# join data in single dataframe\n",
        "df_image = pd.read_csv(path + \"CUB_200_2011/images.txt\", delimiter = \" \", names = [\"id\", \"path\"])\n",
        "df_bounding = pd.read_csv(path + \"CUB_200_2011/bounding_boxes.txt\", delimiter = \" \", names = [\"id\", \"x\", \"y\", \"w\", \"h\"])\n",
        "df_train_test_split = pd.read_csv(path + \"CUB_200_2011/train_test_split.txt\", delimiter = \" \", names = [\"id\",\"split\"])\n",
        "df = pd.merge(pd.merge(df_image, df_bounding) , df_train_test_split)\n",
        "\n",
        "# crop and save images\n",
        "for index, row in df.iterrows():\n",
        "    id, x, y, w, h, split = str(row[\"id\"]), float(row[\"x\"]), float(row[\"y\"]), float(row[\"w\"]), float(row[\"h\"]), float(row[\"split\"]) # 1 train 0 test\n",
        "    img_path = str(row[\"path\"])\n",
        "    im = Image.open(path + \"CUB_200_2011/images/\" + row[\"path\"])\n",
        "    img_dir, img_name = img_path.split(\"/\")\n",
        "    img = im.crop((x, y, x + w, y + h))\n",
        "    if split == 0:\n",
        "        makedir(path + 'ProtoPNet-master/datasets/cub200_cropped/test_cropped/' + img_dir)\n",
        "        img.save(path + 'ProtoPNet-master/datasets/cub200_cropped/test_cropped/' + img_path[:-4] + '.jpg', 'JPEG')\n",
        "    else:\n",
        "        makedir(path + 'ProtoPNet-master/datasets/cub200_cropped/train_cropped/' + img_dir)\n",
        "        img.save(path + 'ProtoPNet-master/datasets/cub200_cropped/train_cropped/' + img_path[:-4] + '.jpg', 'JPEG')\n",
        "\n",
        "# augment data\n",
        "\n",
        "# setup path\n",
        "datasets_root_dir = path + 'ProtoPNet-master/datasets/cub200_cropped/'\n",
        "dir = datasets_root_dir + 'train_cropped/'\n",
        "target_dir = datasets_root_dir + 'train_cropped_augmented/'\n",
        "makedir(target_dir)\n",
        "folders = [os.path.join(dir, folder) for folder in next(os.walk(dir))[1]]\n",
        "target_folders = [os.path.join(target_dir, folder) for folder in next(os.walk(dir))[1]]\n",
        "\n",
        "# data augmentation\n",
        "for i in range(len(folders)):\n",
        "    fd = folders[i]\n",
        "    tfd = target_folders[i]\n",
        "    # rotation\n",
        "    p = Augmentor.Pipeline(source_directory=fd, output_directory=tfd)\n",
        "    p.rotate(probability=1, max_left_rotation=15, max_right_rotation=15)\n",
        "    p.flip_left_right(probability=0.5)\n",
        "    for i in range(10):\n",
        "        p.process()\n",
        "    del p\n",
        "    # skew\n",
        "    p = Augmentor.Pipeline(source_directory=fd, output_directory=tfd)\n",
        "    p.skew(probability=1, magnitude=0.2)  # max 45 degrees\n",
        "    p.flip_left_right(probability=0.5)\n",
        "    for i in range(10):\n",
        "        p.process()\n",
        "    del p\n",
        "    # shear\n",
        "    p = Augmentor.Pipeline(source_directory=fd, output_directory=tfd)\n",
        "    p.shear(probability=1, max_shear_left=10, max_shear_right=10)\n",
        "    p.flip_left_right(probability=0.5)\n",
        "    for i in range(10):\n",
        "        p.process()\n",
        "    del p\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_architecture = 'vgg19'\n",
        "img_size = 224\n",
        "prototype_shape = (2000, 128, 1, 1)\n",
        "num_classes = 200\n",
        "prototype_activation_function = 'log'\n",
        "add_on_layers_type = 'regular'\n",
        "\n",
        "experiment_run = '003'\n",
        "\n",
        "data_path = path + 'ProtoPNet-master/datasets/cub200_cropped/'\n",
        "train_dir = data_path + 'train_cropped_augmented/'\n",
        "test_dir = data_path + 'test_cropped/'\n",
        "train_push_dir = data_path + 'train_cropped/'\n",
        "train_batch_size = 80\n",
        "test_batch_size = 100\n",
        "train_push_batch_size = 75\n",
        "\n",
        "joint_optimizer_lrs = {'features': 1e-4,\n",
        "                       'add_on_layers': 3e-3,\n",
        "                       'prototype_vectors': 3e-3}\n",
        "joint_lr_step_size = 5\n",
        "\n",
        "warm_optimizer_lrs = {'add_on_layers': 3e-3,\n",
        "                      'prototype_vectors': 3e-3}\n",
        "\n",
        "last_layer_optimizer_lr = 1e-4\n",
        "\n",
        "coefs = {\n",
        "    'crs_ent': 1,\n",
        "    'clst': 0.8,\n",
        "    'sep': -0.08,\n",
        "    'l1': 1e-4,\n",
        "}\n",
        "\n",
        "num_train_epochs = 1000\n",
        "num_warm_epochs = 5\n",
        "\n",
        "push_start = 10\n",
        "push_epochs = [i for i in range(num_train_epochs) if i % 10 == 0]\n"
      ],
      "metadata": {
        "id": "EJOZ5LentJ6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}